#!/usr/bin/env python3
"""
ã‚³ãƒ©ãƒ EDæ³•
columnar_ed_ann.py version: 1.033
"""

ã€v032ã®æ›´æ–°å†…å®¹ã€‘(2026-01-04)
â–  ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçŸ©å½¢ç”»åƒè¡¨ç¤ºå¯¾å¿œ ğŸ¯
  - data_loader.py: load_custom_dataset()ã®æˆ»ã‚Šå€¤ã«input_shapeã‚’è¿½åŠ 
  - visualization_manager.py: input_shapeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œ
    * __init__()ã«input_shapeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ 
    * å…¥åŠ›å±¤è¡¨ç¤ºã§input_shapeå„ªå…ˆä½¿ç”¨ï¼ˆçŸ©å½¢ç”»åƒæ­£ã—ãè¡¨ç¤ºï¼‰
  - columnar_ed_ann_v032.py: input_shapeå–å¾—ãƒ»VisualizationManagerã¸æ¸¡ã™
  
  æ¤œè¨¼çµæœ:
  - âœ… 50Ã—30çŸ©å½¢ç”»åƒ: æ­£å¸¸è¡¨ç¤ºç¢ºèª
  - âœ… 100Ã—200çŸ©å½¢ç”»åƒ: æ­£å¸¸è¡¨ç¤ºç¢ºèª
  - âœ… MNIST/CIFAR-10ã¨ã®äº’æ›æ€§ç¶­æŒ
  
  è©³ç´°: CUSTOM_RECTANGLE_IMAGE_VERIFICATION_REPORT.md å‚ç…§

ã€é‡è¦ãªç™ºè¦‹ã€‘(2026-01-02) ğŸ¯
ã‚³ãƒ©ãƒ EDæ³•ãŒãƒªã‚¶ãƒ¼ãƒã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆReservoir Computingï¼‰ã®åŸç†ã§å‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’å®Ÿè¨¼çš„ã«ç™ºè¦‹

â–  ç™ºè¦‹ã®çµŒç·¯:
  1. column_neurons=1è¨­å®šã§81.6%ç²¾åº¦é”æˆï¼ˆ512ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ä¸­10å€‹ã®ã¿å­¦ç¿’ï¼‰
  2. n_hidden=10ã¸ã®å‰Šæ¸›ã§ç²¾åº¦ãŒ12.6%ã«æ¿€æ¸›ï¼ˆ-59.7ãƒã‚¤ãƒ³ãƒˆï¼‰
  3. â†’ 502å€‹ã®éå­¦ç¿’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒé«˜æ¬¡å…ƒå°„å½±ã«è²¢çŒ®ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

â–  ãƒªã‚¶ãƒ¼ãƒã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ã®ä¸»è¦ãªé¡ä¼¼ç‚¹:

  ã€1. å›ºå®šãƒ©ãƒ³ãƒ€ãƒ é‡ã¿ã®æ´»ç”¨ã€‘
    - RC: ãƒªã‚¶ãƒ¼ãƒå±¤å…¨ä½“ã‚’å›ºå®š
    - ã‚³ãƒ©ãƒ ED: 502/512ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆ98%ï¼‰ã‚’å›ºå®š
    - å®Ÿè¨¼: éå­¦ç¿’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é‡ã¿å¤‰åŒ– = 0.000000ï¼ˆå®Œå…¨ã«ã‚¼ãƒ­ï¼‰
  
  ã€2. é«˜æ¬¡å…ƒå°„å½±ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Šã€‘
    - 10ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã¿: ãƒ†ã‚¹ãƒˆç²¾åº¦ 12.6%
    - 512ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆ10å€‹ã®ã¿å­¦ç¿’ï¼‰: ãƒ†ã‚¹ãƒˆç²¾åº¦ 72.3%
    - å·®åˆ†: +59.7ãƒã‚¤ãƒ³ãƒˆ â†’ ãƒ©ãƒ³ãƒ€ãƒ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ˜ç¢ºãªè²¢çŒ®ã‚’å®Ÿè¨¼
  
  ã€3. å­¦ç¿’ã®å±€æ‰€åŒ–ã€‘
    - RC: èª­ã¿å‡ºã—å±¤ï¼ˆreadout layerï¼‰ã®ã¿å­¦ç¿’
    - ã‚³ãƒ©ãƒ ED: 10ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ + å‡ºåŠ›å±¤ã®ã¿å­¦ç¿’ï¼ˆå…¨ä½“ã®2%ï¼‰
    - è¨ˆç®—åŠ¹ç‡: O(10 Ã— 784) backward â‰ˆ 2%ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°

â–  ç†è«–çš„ä½ç½®ã¥ã‘:
  
  Random Projectionç†è«–
       â†“
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
  â†“          â†“
  RC      Extreme Learning Machine (ELM)
  â†“          â†“
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â†“
  **ã‚³ãƒ©ãƒ EDæ³•**
  (+ ç”Ÿç‰©å­¦çš„åˆ¶ç´„ + ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¬ãƒ™ãƒ«é¸æŠ)

  ç‰¹ã«Extreme Learning Machineï¼ˆELMï¼‰ã¨æ§‹é€ ãŒé…·ä¼¼:
  - éš ã‚Œå±¤: ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã—ã¦å›ºå®š
  - å‡ºåŠ›å±¤: å­¦ç¿’å¯èƒ½
  - é™çš„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã«ç‰¹åŒ–

â–  ã‚³ãƒ©ãƒ EDæ³•ã®ç‹¬è‡ªæ€§: "Embedded Reservoir"ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
  
  å¾“æ¥ã®RCï¼ˆå±¤ãƒ¬ãƒ™ãƒ«ã®åˆ†é›¢ï¼‰:
    [å…¥åŠ›] â†’ [ãƒªã‚¶ãƒ¼ãƒå±¤:å…¨å›ºå®š] â†’ [èª­ã¿å‡ºã—å±¤:å…¨å­¦ç¿’] â†’ [å‡ºåŠ›]
  
  ã‚³ãƒ©ãƒ EDæ³•ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¬ãƒ™ãƒ«ã®æ··åœ¨ï¼‰:
    [å…¥åŠ›] â†’ [æ··åˆå±¤: 502å›ºå®š + 10å­¦ç¿’] â†’ [å‡ºåŠ›å±¤] â†’ [å‡ºåŠ›]
              â””â”€ åŒã˜å±¤å†…ã«å­¦ç¿’/éå­¦ç¿’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå…±å­˜
  
  é©æ–°æ€§:
  - RCã¯ã€Œå±¤ãƒ¬ãƒ™ãƒ«ã€ã®åˆ†é›¢
  - ã‚³ãƒ©ãƒ EDæ³•ã¯ã€Œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¬ãƒ™ãƒ«ã€ã®åˆ†é›¢
  - ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§ï¼ˆå¤§è„³çš®è³ªã‚³ãƒ©ãƒ ï¼‰ã‚’ç¶­æŒ

â–  å‚è€ƒæ–‡çŒ®ãƒ»é–¢é€£æŠ€è¡“:
  - Echo State Network (Jaeger, 2001)
  - Liquid State Machine (Maass et al., 2002)
  - Extreme Learning Machine (Huang et al., 2006)
  - Random Kitchen Sinks (Rahimi & Recht, 2008)
  - Physical Reservoir Computing (Nature, 2021)

è©³ç´°åˆ†æ: RESERVOIR_COMPUTING_COMPARISON.md å‚ç…§

ã€v031ã«ã¤ã„ã¦ã€‘(2026-01-01ä½œæˆ)
v030ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã•ã‚ŒãŸé–‹ç™ºç”¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
v030ã§é”æˆã—ãŸ85%ã®å£çªç ´ï¼ˆTestç²¾åº¦84.67%ï¼‰ãŠã‚ˆã³æ–¹æ³•Cå®Ÿè£…ã®å…¨æ©Ÿèƒ½ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã™ã€‚

ã€v030ã®æˆæœã€‘(2025-12-29ä½œæˆ - 2026-01-01ç¢ºå®š)
â–  85%ã®å£çªç ´ï¼ˆæœ€çµ‚å®‰å®šç‰ˆï¼‰
  - Best Testç²¾åº¦: 84.67% (Epoch 28, PR=0.05)
  - æœ€é©PRå€¤ç™ºè¦‹: participation_rate=0.05ãŒæœ€é«˜æ€§èƒ½
  - ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æœ€å°åŒ–: Layer2ã§2.3% (6/256å€‹)
  - ã‚³ãƒ©ãƒ æ§‹é€ æœ€é©åŒ–: å„ã‚¯ãƒ©ã‚¹2-3å€‹ï¼ˆè¶…ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ï¼‰
  
â–  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«:
  * columnar_ed_ann_v030_backup_learning_success.py
  * modules_v030_backup_learning_success/
  
â–  ä¸»è¦å®Ÿè£…:
  * æ–¹æ³•Cï¼ˆTop-K + æ®‹å·®affinityï¼‰ã«ã‚ˆã‚‹å…¨å€™è£œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å­¦ç¿’å‚åŠ 
  * PRæœ€é©åŒ–ï¼ˆ0.05ãŒæœ€é©å€¤ï¼‰
  * å°‘æ•°ç²¾é‹­åŸç†ï¼ˆElite Selectionï¼‰ã®å®Ÿè¨¼

ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±ã€‘(2026-01-01)
â–  å­¦ç¿’æˆåŠŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆv030 æœ€çµ‚å®‰å®šç‰ˆï¼‰
  - çŠ¶æ…‹: æ–¹æ³•Cæœ€é©åŒ–å®Œäº†ã€85%ã®å£çªç ´ï¼ˆ84.67%é”æˆï¼‰
  - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«:
    * columnar_ed_ann_v030_backup_learning_success.py
    * modules_v030_backup_learning_success/
  - æˆæœ:
    * 85%ã®å£çªç ´: Best Testç²¾åº¦ 84.67% (Epoch 28)
    * æœ€é©PRå€¤ç™ºè¦‹: participation_rate=0.05ãŒæœ€é«˜æ€§èƒ½
    * ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æœ€å°åŒ–: Layer2ã§2.3% (6/256å€‹)
    * ã‚³ãƒ©ãƒ æ§‹é€ æœ€é©åŒ–: å„ã‚¯ãƒ©ã‚¹2-3å€‹ï¼ˆè¶…ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ï¼‰
  - å®Ÿè£…å†…å®¹: æ–¹æ³•Cï¼ˆTop-K + æ®‹å·®affinityï¼‰ã«ã‚ˆã‚‹å…¨å€™è£œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å­¦ç¿’å‚åŠ 
  - è©³ç´°: ä¸‹è¨˜ã€Œv030ã®ä¸»è¦å®Ÿè£…ã€ãŠã‚ˆã³ã€Œv030å­¦ç¿’æˆåŠŸå®Ÿç¸¾ã€å‚ç…§

â–  æ–¹æ³•Cï¼ˆTop-K + æ®‹å·®affinityï¼‰å®Ÿè£…æˆåŠŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã€çµ±åˆæ¸ˆã€‘
  - çŠ¶æ…‹: æ–¹æ³•Cã«ã‚ˆã‚Šå…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå­¦ç¿’å‚åŠ å¯èƒ½ã€Testç²¾åº¦70.7%é”æˆ
  - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«:
    * columnar_ed_ann_v030_backup_method_c_success_20260101_190807.py
    * modules_backup_method_c_success_20260101_190807/
  - æˆæœ: ã‚³ãƒ©ãƒ æ§‹é€ ã‚’ç¶­æŒã—ã¤ã¤å­¦ç¿’æˆåŠŸï¼ˆ8.7% â†’ 70.7%ï¼‰

ã€v030ã«ã¤ã„ã¦ã€‘(2025-12-29ä½œæˆ)
v029ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã•ã‚ŒãŸé–‹ç™ºç”¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
v029ã®å…¨æ©Ÿèƒ½ï¼ˆãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’å®Ÿè£…ã€--wiså¼•æ•°ã€HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«æ‹¡å¼µã€u1/u2ãƒã‚°ä¿®æ­£ç­‰ï¼‰ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã™ã€‚

ã€v029ã«ã¤ã„ã¦ã€‘(2025-12-25ä½œæˆ - 2025-12-29ç¢ºå®š)
v028ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚
v028ã®å…¨æ©Ÿèƒ½ï¼ˆ--wiså¼•æ•°ã€HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«æ‹¡å¼µã€u1/u2ãƒã‚°ä¿®æ­£ç­‰ï¼‰ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã™ã€‚

ã€v029ã®ä¸»è¦å®Ÿè£…ã€‘
â–  ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®å®Ÿè£…ï¼ˆ2025-12-28ï¼‰
  - modules/ed_network.py ã« train_epoch_minibatch_tf() ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…
  - TensorFlow Dataset APIä½¿ç”¨ï¼ˆæ¥­ç•Œæ¨™æº–æ‰‹æ³•ï¼‰
  - å‹¾é…å¹³å‡åŒ–æ–¹å¼ï¼ˆPyTorchæ–¹å¼ï¼‰æ¡ç”¨
    * ãƒãƒƒãƒå†…ã®å…¨ã‚µãƒ³ãƒ—ãƒ«ã®å‹¾é…ã‚’è“„ç©
    * å‹¾é…ã‚’å¹³å‡åŒ–ï¼ˆÃ· batch_sizeï¼‰
    * å¹³å‡åŒ–ã•ã‚ŒãŸå‹¾é…ã§ä¸€åº¦ã ã‘é‡ã¿æ›´æ–°
    * å®ŸåŠ¹å­¦ç¿’ç‡ = lr ï¼ˆbatch_sizeã«ä¾å­˜ã—ãªã„ï¼‰
  
  - å®Ÿè£…ä¸Šã®è©¦è¡ŒéŒ¯èª¤:
    * v031è©¦è¡Œï¼ˆ2025-12-28ï¼‰: å¹³å‡åŒ–å¾Œã«batch_sizeå€ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
      â†’ å®ŸåŠ¹lr = lr Ã— batch_size ã¨ãªã‚Šã€batch_size=64ã§å­¦ç¿’ä¸å¯èƒ½ï¼ˆTestç²¾åº¦9.03%ã§åœæ»ï¼‰
      â†’ PyTorchæ–¹å¼ã«æˆ»ã™ã“ã¨ã§è§£æ±ºï¼ˆv030ã¨ã—ã¦å®Ÿè£…ï¼‰
    * v030æœ€çµ‚å®Ÿè£…ï¼ˆ2025-12-28ï¼‰: å‹¾é…å¹³å‡åŒ–ã®ã¿ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãªã—ï¼‰
      â†’ batch_size=64ã§æ­£å¸¸ã«å­¦ç¿’ï¼ˆ10ã‚¨ãƒãƒƒã‚¯ã§52.33%é”æˆï¼‰
  
  - å®Ÿé¨“çµæœï¼ˆ2025-12-28 - 2025-12-29ã€hidden=[512,256], lr=0.1, 100ã‚¨ãƒãƒƒã‚¯ï¼‰:
    * ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’: Test=83.40% (Ep.23), Train=93.13%
    * ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’(batch=64): Test=74.50% (Ep.98), Train=80.03%
    * å·®åˆ†: -8.90%ï¼ˆãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ãŒåŠ£ã‚‹çµæœï¼‰
  
  - ä»Šå¾Œã®èª²é¡Œ:
    * ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ãŒã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã«åŠ£ã‚‹åŸå› ã®è§£æ˜
    * å®Ÿè£…ã®è¦‹ç›´ã—ãŒå¿…è¦ï¼ˆå°†æ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å¯¾å¿œäºˆå®šï¼‰

ã€v030ã®ä¸»è¦å®Ÿè£…ã€‘
â–  Affinityæ­£è¦åŒ–ï¼ˆæ–¹æ³•C: Top-K + æ®‹å·®affinityï¼‰å®Ÿè£…ï¼ˆ2026-01-01ï¼‰
  - ç›®çš„: å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’å­¦ç¿’ã«å‚åŠ ã•ã›ã€85%ç²¾åº¦ã®å£ã‚’çªç ´
  - å®Ÿè£…å ´æ‰€: modules/column_structure.py ã® create_column_affinity_honeycomb()
  
  - æ–¹æ³•Cã®è¨­è¨ˆæ€æƒ³:
    * è·é›¢ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: å„ã‚¯ãƒ©ã‚¹ã«ã¤ãç´„51å€‹ï¼ˆ10%ï¼‰ã®å€™è£œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’é¸å‡º
    * Top-Ké¸æŠ: å€™è£œã®ä¸­ã‹ã‚‰ä¸Šä½Kå€‹ï¼ˆç´„5å€‹ï¼‰ã«å¼·ã„affinityä»˜ä¸
    * æ®‹å·®affinity: æ®‹ã‚Šã®å€™è£œï¼ˆç´„46å€‹ï¼‰ã«å¼±ã„affinityä»˜ä¸ï¼ˆTop-Kã®2%ï¼‰
    * éå€™è£œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: affinity=0ï¼ˆä»–ã‚¯ãƒ©ã‚¹ã«å¸°å±ï¼‰
  
  - å®Ÿè£…ã®è©¦è¡ŒéŒ¯èª¤:
    * æ–¹æ³•Aï¼ˆSoftmaxæ­£è¦åŒ–ï¼‰:
      - å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å‡ä¸€ãªaffinityä»˜ä¸ï¼ˆtemperature=0.5ï¼‰
      - çµæœ: ã‚³ãƒ©ãƒ æ§‹é€ ãŒç ´å£Šã•ã‚Œã€Testç²¾åº¦8.7%ã§åœæ»
      - å•é¡Œ: å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå…¨ã‚¯ãƒ©ã‚¹ã«å¸°å±ï¼ˆaffinity â‰ˆ 0.001ï¼‰
    
    * æ–¹æ³•Bï¼ˆæœ€å°å€¤ä¿è¨¼ï¼‰:
      - affinity=0ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«æœ€å°å€¤ã®1%ã‚’ä»˜ä¸
      - çµæœ: ã‚³ãƒ©ãƒ æ§‹é€ ã¯ç¶­æŒã•ã‚ŒãŸãŒã€Testç²¾åº¦8.7%ã§åœæ»
      - å•é¡Œ: å®ŸåŠ¹å‚åŠ ç‡ãŒ10%ã®ã¾ã¾å¤‰ã‚ã‚‰ãš
    
    * æ–¹æ³•Cï¼ˆTop-K + æ®‹å·®affinityï¼‰:
      - ç¬¬1å®Ÿè£…ï¼ˆ2026-01-01åˆå‰ï¼‰: å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰ç›´æ¥Top-Ké¸æŠ
        â†’ å•é¡Œ: å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå…¨ã‚¯ãƒ©ã‚¹ã«å¸°å±ï¼ˆå„ã‚¯ãƒ©ã‚¹512å€‹è¡¨ç¤ºï¼‰
        â†’ åŸå› : è·é›¢ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã„ãŸ
      
      - ç¬¬2å®Ÿè£…ï¼ˆ2026-01-01åˆå¾Œï¼‰: è·é›¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿å€™è£œã‹ã‚‰Top-Ké¸æŠ
        â†’ æˆåŠŸ: ã‚³ãƒ©ãƒ æ§‹é€ ç¶­æŒã€Testç²¾åº¦70.7%é”æˆï¼ˆ5ã‚¨ãƒãƒƒã‚¯ï¼‰
        â†’ åŠ¹æœ: å‡¦ç†æ™‚é–“47ç§’â†’7ç§’ï¼ˆ85%é«˜é€ŸåŒ–ï¼‰
  
  - å®Ÿè£…ã®è©³ç´°ï¼ˆmodules/column_structure.py Line 299-370ï¼‰:
    1. è·é›¢ãƒ™ãƒ¼ã‚¹ã§affinityå€™è£œã‚’è¨ˆç®—ï¼ˆæ—¢å­˜å®Ÿè£…ï¼‰
    2. å„ã‚¯ãƒ©ã‚¹ã®å€™è£œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼ˆaffinity>1e-10ï¼‰ã‚’æŠ½å‡º
    3. å€™è£œã®ä¸­ã‹ã‚‰Top-Kå€‹ã‚’é¸æŠï¼ˆK = n_hidden * participation_rate / n_classesï¼‰
    4. Top-K: å…ƒã®affinityå€¤ã‚’ä¿æŒï¼ˆå¼·ã„å­¦ç¿’ï¼‰
    5. æ®‹ã‚Šå€™è£œ: å¼±affinity = Top-Kå¹³å‡ã®2%
    6. target_sumã«åˆã‚ã›ã¦å†æ­£è¦åŒ–
  
  - æ¤œè¨¼çµæœï¼ˆ1000ã‚µãƒ³ãƒ—ãƒ«ã€5ã‚¨ãƒãƒƒã‚¯ï¼‰:
    * Testç²¾åº¦: 70.7% ï¼ˆæ–¹æ³•A/B: 8.7%ï¼‰
    * å‡¦ç†æ™‚é–“: 7ç§’/ã‚¨ãƒãƒƒã‚¯ ï¼ˆæ–¹æ³•A/B: 47ç§’/ã‚¨ãƒãƒƒã‚¯ï¼‰
    * ã‚³ãƒ©ãƒ æ§‹é€ : å„ã‚¯ãƒ©ã‚¹5-6å€‹ ï¼ˆæ–¹æ³•A/B: 512å€‹=ç ´å£Šï¼‰
    * Affinityåˆ†å¸ƒ: Top-Kå¼·affinity + æ®‹å·®å¼±affinity
      - Layer1: é«˜affinity 30/512 (5.9%), å„ã‚¯ãƒ©ã‚¹5-6å€‹
      - Layer2: é«˜affinity 20/256 (7.8%), å„ã‚¯ãƒ©ã‚¹2-3å€‹
    * ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: Layer1=0/512, Layer2=11/256 (4.3%)
  
ã€v030å­¦ç¿’æˆåŠŸå®Ÿç¸¾ã€‘(2026-01-01)
â–  85%ã®å£çªç ´æ¤œè¨¼ï¼ˆ3000ã‚µãƒ³ãƒ—ãƒ«ã€30ã‚¨ãƒãƒƒã‚¯ï¼‰
  - æ–¹æ³•Cã«ã‚ˆã‚‹å¤§è¦æ¨¡å­¦ç¿’ã§85%ã®å£ã‚’çªç ´
  - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ: [512, 256] 2å±¤
  - å­¦ç¿’æ¡ä»¶: lr=0.1, u1=1.0, u2=1.5, ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ=MNIST
  
  â–  åˆå›æˆåŠŸï¼ˆPR=0.1ï¼‰:
    * Best Testç²¾åº¦: 84.47% (Epoch 23) â† 85%ã®å£çªç ´
    * æœ€çµ‚Testç²¾åº¦: 84.27% (Epoch 30)
    * è¨“ç·´ç²¾åº¦: 92.77%
    * ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: Layer1=0/512, Layer2=11/256 (4.3%)
    * ã‚³ãƒ©ãƒ æ§‹é€ : å„ã‚¯ãƒ©ã‚¹5-6å€‹
    * Affinityåˆ†å¸ƒ:
      - Layer1: é«˜affinity 30/512 (5.9%), éã‚¼ãƒ­51è¦ç´ 
      - Layer2: é«˜affinity 20/256 (7.8%), éã‚¼ãƒ­40è¦ç´ 
  
  â–  Participation Rate (PR) æœ€é©å€¤æ¢ç´¢:
    - ç›®çš„: 85%ã®å£çªç ´ã‚’å®‰å®šåŒ–ã™ã‚‹æœ€é©PRå€¤ã®ç™ºè¦‹
    - å®Ÿé¨“ç¯„å›²: PR = 0.05, 0.1, 0.2, 0.3, 0.71
    - å®Ÿé¨“æ¡ä»¶: 3000ã‚µãƒ³ãƒ—ãƒ«ã€30ã‚¨ãƒãƒƒã‚¯ã€åŒä¸€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    | PRå€¤ | å„ã‚¯ãƒ©ã‚¹Neuronæ•°(L1) | Best Testç²¾åº¦ | Epoch | Dead L2 | 85%å£çªç ´ |
    |------|---------------------|--------------|-------|---------|----------|
    | 0.05 | 2-3å€‹ | **84.67%** | 28 | 6 (2.3%) | âœ… |
    | 0.1  | 5-6å€‹ | **84.47%** | 23 | 11 (4.3%) | âœ… |
    | 0.2  | 10-11å€‹ | 83.77% | 30 | 19 (7.4%) | âŒ |
    | 0.3  | 15-16å€‹ | 83.50% | 28 | 22 (8.6%) | âŒ |
    | 0.71 | 30å€‹ | 83.57% | 29 | 33 (12.9%) | âŒ |
  
  â–  ä¸»è¦ç™ºè¦‹:
    1. **æœ€é©PRå€¤: 0.05** ï¼ˆå…¨å®Ÿé¨“ä¸­æœ€é«˜ç²¾åº¦84.67%ï¼‰
       * å„ã‚¯ãƒ©ã‚¹2-3å€‹ã®è¶…ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾
       * ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æœ€å°ï¼ˆ2.3%ï¼‰
       * å‡¦ç†æ™‚é–“æœ€é€Ÿï¼ˆ18ç§’/epochï¼‰
       * ç”Ÿç‰©å­¦çš„å¦¥å½“æ€§ãŒé«˜ã„ï¼ˆè„³ã®ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ã¨ä¸€è‡´ï¼‰
    
    2. **PRå¢—åŠ  â†’ æ€§èƒ½ä½ä¸‹ã®æ³•å‰‡**
       * PR â†‘ â†’ Testç²¾åº¦ â†“ï¼ˆåæ¯”ä¾‹é–¢ä¿‚ï¼‰
       * PR â†‘ â†’ ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ â†‘ï¼ˆç·šå½¢é–¢ä¿‚ï¼‰
       * PR â†‘ â†’ å‡¦ç†æ™‚é–“ â†‘
    
    3. **å°‘æ•°ç²¾é‹­åŸç† (Elite Selection)**
       * ã‚¹ãƒ‘ãƒ¼ã‚¹è¡¨ç¾ = é«˜åŠ¹ç‡ = é«˜ç²¾åº¦
       * éåº¦ãªå‚åŠ  = ç«¶åˆæ¿€åŒ– = æ·˜æ±°ç™ºç”Ÿ
       * è„³ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨åŒã˜åŸç†
    
    4. **åˆæœŸå­¦ç¿’é€Ÿåº¦ã®å„ªä½æ€§**
       | PRå€¤ | Epoch 1 Testç²¾åº¦ | å·®åˆ† (vs PR=0.05) |
       |------|-----------------|-------------------|
       | 0.05 | 73.40% | - |
       | 0.1  | 72.20% | -1.20% |
       | 0.2  | 68.70% | -4.70% |
       | 0.3  | 67.90% | -5.50% |
       | 0.71 | 63.20% | -10.20% |
       * ä½PR = åˆæœŸã‹ã‚‰é«˜ç²¾åº¦
  
  â–  æ¨å¥¨è¨­å®š:
    * **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆPRå€¤: 0.05** ï¼ˆæœ€é«˜ç²¾åº¦ã€æœ€å°ãƒ‡ãƒƒãƒ‰ã€æœ€é«˜åŠ¹ç‡ï¼‰
    * æ¨å¥¨ç¯„å›²: 0.05 - 0.1 ï¼ˆ85%ã®å£çªç ´å¯èƒ½ï¼‰
    * éæ¨å¥¨ç¯„å›²: 0.2ä»¥ä¸Š ï¼ˆæ€§èƒ½ä½ä¸‹ã€ãƒ‡ãƒƒãƒ‰å¢—åŠ ï¼‰
  
  â–  ä»Šå¾Œã®èª²é¡Œ:
    * PR=0.03, 0.07ã§ã®å¾®èª¿æ•´ï¼ˆ0.05å‘¨è¾ºã®æœ€é©åŒ–ï¼‰
    * å±¤åˆ¥PRæœ€é©åŒ–ï¼ˆLayer1ã¨Layer2ã§ç•°ãªã‚‹PRå€¤ï¼‰
    * å‹•çš„PRèª¿æ•´ï¼ˆå­¦ç¿’é€²è¡Œã«å¿œã˜ãŸPRå¤‰æ›´ï¼‰
    * å¤§è¦æ¨¡æ¤œè¨¼ï¼ˆ10000ã‚µãƒ³ãƒ—ãƒ«ã€100ã‚¨ãƒãƒƒã‚¯ã§ã®å†ç¾æ€§ç¢ºèªï¼‰
    * weak_affinity_ratioï¼ˆç¾åœ¨0.02ï¼‰ã®æœ€é©å€¤æ¢ç´¢

ã€ç¶™æ‰¿æ©Ÿèƒ½ä¸€è¦§ã€‘
â–  é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³æŒ‡å®šæ©Ÿèƒ½ (v028)
  - --wiså¼•æ•°ã«ã‚ˆã‚‹å±¤ã”ã¨ã®é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã®æŸ”è»ŸãªæŒ‡å®š
  - ç¹°ã‚Šè¿”ã—è¨˜æ³•: å€¤[å›æ•°] ã«ã‚ˆã‚‹è¶…å¤šå±¤å¯¾å¿œ
  - å„ªå…ˆé †ä½: CLIå¼•æ•° > HyperParamsãƒ†ãƒ¼ãƒ–ãƒ« > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

â–  HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã®æ‹¡å¼µ (v028)
  - å±¤ã”ã¨ã®é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿æŒ
  - å®Ÿé¨“024 Phase 1-3ã®æœ€é©åŒ–çµæœã‚’åæ˜ 

â–  u1/u2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒã‚°ä¿®æ­£ (v028)
  - ã‚¢ãƒŸãƒ³æ‹¡æ•£å‡¦ç†ã®2ã‚¹ãƒ†ãƒƒãƒ—å‡¦ç†å®Ÿè£…
  - ã‚ªãƒªã‚¸ãƒŠãƒ«Cã‚³ãƒ¼ãƒ‰æº–æ‹ ã®å‹•ä½œã‚’å®Ÿç¾

â–  ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ (v027.3)
  - å•é¡Œ: ã‚¢ãƒŸãƒ³æ‹¡æ•£å‡¦ç†ã§column_affinityã¨ã®ç©ç®—é †åºãŒèª¤ã£ã¦ã„ãŸ
  - å½±éŸ¿: v027ä»¥å‰ã§ã¯ã€u1/u2ã®å€¤ã‚’å¤‰æ›´ã—ã¦ã‚‚å­¦ç¿’ç²¾åº¦ãŒå¤‰åŒ–ã—ãªã„ï¼ˆåŠ¹æœãŒæ¸›è¡°ï¼‰
  - åŸå› : modules/ed_network.pyã§ `amine Ã— u1 Ã— column_affinity` ã®é †ã§è¨ˆç®—
  - ä¿®æ­£: ã‚ªãƒªã‚¸ãƒŠãƒ«Cã‚³ãƒ¼ãƒ‰æº–æ‹ ã®2ã‚¹ãƒ†ãƒƒãƒ—å‡¦ç†ã«å¤‰æ›´
    * ã‚¹ãƒ†ãƒƒãƒ—1: `amine_diffused = amine_concentration Ã— diffusion_coef`ï¼ˆå…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ä¸€å¾‹ï¼‰
    * ã‚¹ãƒ†ãƒƒãƒ—2: `amine_hidden_3d = amine_diffused Ã— column_affinity`ï¼ˆã‚³ãƒ©ãƒ æ§‹é€ ã§é‡ã¿ä»˜ã‘ï¼‰
  - åŠ¹æœ: u1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤‰åŒ–ãŒç²¾åº¦ã«æ­£ã—ãåæ˜ ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    * u1=0.3: Test=0.746 (10epoch)
    * u1=0.5: Test=0.743 (10epoch, -0.4%)
    * u1=0.7: Test=0.742 (10epoch, -0.5%) â†’ 50epoch: Test=0.780
  - å‚è€ƒ: original-c-source-code/teach_calc.c 26-27è¡Œç›®ã®å®Ÿè£…ã«æº–æ‹ 

ã€v027.3ã®æ–°æ©Ÿèƒ½ã€‘
v027.2ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã•ã‚ŒãŸé–‹ç™ºç”¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã™ã€‚

Phase 3å®Ÿè£…ï¼ˆ2025-12-20ï¼‰: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ– âœ…
  â–  ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–
    - metadata.json: è©³ç´°ãªJSONè§£æã‚¨ãƒ©ãƒ¼ï¼ˆè¡Œç•ªå·ã€åˆ—ç•ªå·ï¼‰ã€å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œè¨¼ã€å‹ãƒã‚§ãƒƒã‚¯
    - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹: é¡ä¼¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå€™è£œã®è‡ªå‹•ææ¡ˆã€ä½¿ç”¨æ–¹æ³•ã®ã‚¬ã‚¤ãƒ‰è¡¨ç¤º
    - ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼: NaN/Infä½ç½®ç‰¹å®šã€ç¯„å›²å¤–ãƒ©ãƒ™ãƒ«è©³ç´°ã€å…·ä½“çš„ãªä¿®æ­£æ–¹æ³•ã®æç¤º
    - ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬è¡¨ç¤ºã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±
  
  â–  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
    - ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰: 100MBä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è‡ªå‹•é©ç”¨ï¼ˆnp.load mmap_mode='r'ï¼‰
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ã®æ—©æœŸé©ç”¨ã€ä¸è¦ãªã‚³ãƒ”ãƒ¼ã®å‰Šæ¸›
    - é€²æ—è¡¨ç¤º: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿æ™‚ã®çŠ¶æ…‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  
  â–  ã‚¯ãƒ©ã‚¹åè¡¨ç¤ºæ©Ÿèƒ½ã®æ‹¡å¼µ
    - metadata.jsonã«class_namesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    - æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆMNIST, Fashion-MNIST, CIFAR-10ï¼‰ã®ã‚¯ãƒ©ã‚¹åçµ„ã¿è¾¼ã¿
    - å­¦ç¿’çµæœè¡¨ç¤ºæ™‚ã«ã‚¯ãƒ©ã‚¹åã‚’ä½¿ç”¨
  
  â–  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼æ©Ÿèƒ½
    - ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿ã§è‡ªå‹•å®Ÿè¡Œï¼ˆæ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    - 5ã¤ã®æ¤œè¨¼é …ç›®: ãƒ‡ãƒ¼ã‚¿å‹ã€æ¬ æå€¤ã€ãƒ©ãƒ™ãƒ«ç¯„å›²ã€æ•´åˆæ€§ã€ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
    - è©³ç´°ãªæ¤œè¨¼çµæœã¨ã‚¯ãƒ©ã‚¹ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°è¡¨ç¤º

Phase 2å®Ÿè£…ï¼ˆ2025-12-20ï¼‰: ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ âœ…
  â–  ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    - metadata.jsonå¯¾å¿œã®æŸ”è»Ÿãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
    - .npyå½¢å¼ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    - è‡ªå‹•æ­£è¦åŒ–ãƒ»ãƒ•ãƒ©ãƒƒãƒˆåŒ–æ©Ÿèƒ½
  
  â–  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹è§£æ±ºæ©Ÿèƒ½
    - æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆmnist, fashion, cifar10, cifar100ï¼‰ã®è‡ªå‹•èªè­˜
    - ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æŸ”è»Ÿãªãƒ‘ã‚¹æŒ‡å®š
    - æ¤œç´¢å„ªå…ˆé †ä½: æŒ‡å®šãƒ‘ã‚¹ â†’ ~/.keras/datasets/ â†’ ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
  
  â–  ä½¿ç”¨ä¾‹
    # æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    python columnar_ed_ann_v027_3.py --dataset mnist
    python columnar_ed_ann_v027_3.py --dataset cifar10
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆåå‰æŒ‡å®šï¼‰
    python columnar_ed_ann_v027_3.py --dataset my_custom_data
    # â†’ ~/.keras/datasets/my_custom_data/ ã‚’æ¤œç´¢
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ‘ã‚¹æŒ‡å®šï¼‰
    python columnar_ed_ann_v027_3.py --dataset /path/to/my_data
    
  â–  ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ :
      my_custom_data/
      â”œâ”€â”€ metadata.json       # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰
      â”œâ”€â”€ x_train.npy        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰
      â”œâ”€â”€ y_train.npy        # è¨“ç·´ãƒ©ãƒ™ãƒ«ï¼ˆå¿…é ˆï¼‰
      â”œâ”€â”€ x_test.npy         # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå¿…é ˆï¼‰
      â””â”€â”€ y_test.npy         # ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«ï¼ˆå¿…é ˆï¼‰
    
    metadata.jsonå½¢å¼:
      {
          "name": "my_custom_data",
          "n_classes": 10,
          "input_shape": [28, 28],  // ã¾ãŸã¯ [32, 32, 3]
          "normalize": true,         // 0-255 â†’ 0-1æ­£è¦åŒ–ãŒå¿…è¦ã‹
          "class_names": ["class0", "class1", ...],  // ã‚ªãƒ—ã‚·ãƒ§ãƒ³
          "description": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª¬æ˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
      }

Phase 1å®Ÿè£…ï¼ˆ2025-12-20ï¼‰: åŸºæœ¬è‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½ âœ…
  â–  å…¥åŠ›æ¬¡å…ƒã¨ã‚¯ãƒ©ã‚¹æ•°ã®è‡ªå‹•æ¤œå‡º
    - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«å…¥åŠ›æ¬¡å…ƒã‚’æ¤œå‡ºï¼ˆ784, 3072, etc.ï¼‰
    - ã‚¯ãƒ©ã‚¹æ•°ã‚‚è‡ªå‹•æ¤œå‡ºï¼ˆ10, 100, etc.ï¼‰
    - CIFAR-10ãªã©ã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«è‡ªå‹•å¯¾å¿œ
  
  â–  --datasetå¼•æ•°ã®çµ±ä¸€
    - æ–°å½¢å¼: --dataset mnist, --dataset fashion, --dataset cifar10
    - å¾Œæ–¹äº’æ›æ€§: --fashion ã‚‚å¼•ãç¶šãä½¿ç”¨å¯èƒ½

ã€v027.2ã®ä¸»è¦å¤‰æ›´ç‚¹ã€‘
æœ¬ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ã€TensorFlow Dataset APIä¸€æœ¬åŒ–ã«ã‚ˆã‚Šã€ã‚³ãƒ¼ãƒ‰ã®ç°¡æ½”æ€§ã¨å›½éš›çš„ä¿¡é ¼æ€§ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚

â–  NumPyã‚·ãƒ£ãƒƒãƒ•ãƒ«å®Ÿè£…ã®å®Œå…¨å‰Šé™¤
  - modules/ed_network.py ã‹ã‚‰ train_epoch_minibatch() ãƒ¡ã‚½ãƒƒãƒ‰å‰Šé™¤ï¼ˆ60è¡Œå‰Šæ¸›ï¼‰
  - ã™ã¹ã¦ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ©Ÿèƒ½ã‚’ TensorFlow Dataset API ã«çµ±ä¸€
  - ä¿å®ˆè² è·ã®è»½æ¸›ã€ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦ã®ä½ä¸‹

â–  TensorFlow Dataset APIä¸€æœ¬åŒ–
  - æ¥­ç•Œæ¨™æº–æ‰‹æ³•ã®æ¡ç”¨ã«ã‚ˆã‚‹å›½éš›çš„ä¿¡é ¼æ€§ã®ç¢ºç«‹
  - å­¦ç¿’å®‰å®šæ€§35.6%å‘ä¸Šï¼ˆæ¨™æº–åå·®: NumPy 8.24% â†’ TensorFlow 5.31%ï¼‰
  - ç²¾åº¦åŒç­‰æ€§ã‚’70ã‚¨ãƒãƒƒã‚¯å®Ÿé¨“ã§æ¤œè¨¼ï¼ˆæœ€çµ‚ç²¾åº¦å®Œå…¨ä¸€è‡´: 75.60%ï¼‰
  - seedå¼•æ•°ã¯å¸¸ã«TensorFlow Dataset APIã«æ¸¡ã•ã‚Œã‚‹ï¼ˆå®Œå…¨ãªå†ç¾æ€§ä¿è¨¼ï¼‰
  - è©³ç´°: TENSORFLOW_DATALOADER_GUIDE.md å‚ç…§

â–  ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆã®æ”¹å–„
  - --batch å¼•æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0 â†’ Noneï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãŒã‚ˆã‚Šç›´æ„Ÿçš„ã«ï¼‰
  - --shuffle å¼•æ•°: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã¨ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã®ä¸¡æ–¹ã«å¯¾å¿œ
  - 4ã¤ã®å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰:
    * å¼•æ•°ãªã— â†’ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰
    * --shuffle â†’ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šã€batch_size=1ï¼‰
    * --batch N â†’ ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰
    * --batch N --shuffle â†’ ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šï¼‰

ã€v027ã«ã¤ã„ã¦ã€‘
æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã¯ columnar_ed_ann_v026_multiclass_multilayer_modular_B_simplified.py ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚
v026_B_simplifiedã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€ä»Šå¾Œã®å®Ÿè£…å¤‰æ›´ã¯v027ã§è¡Œã„ã¾ã™ã€‚

ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ:
  - modules/hyperparameters.py: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
  - modules/data_loader.py: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆTensorFlow Data APIçµ±åˆã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
  - modules/activation_functions.py: æ´»æ€§åŒ–é–¢æ•°
  - modules/neuron_structure.py: E/Iãƒšã‚¢æ§‹é€ 
  - modules/amine_diffusion.py: ã‚¢ãƒŸãƒ³æ‹¡æ•£
  - modules/column_structure.py: ã‚³ãƒ©ãƒ æ§‹é€ 
  - modules/ed_network.py: ãƒ¡ã‚¤ãƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆTensorFlowä¸€æœ¬åŒ–ã€NumPyå‰Šé™¤ï¼‰
  - modules/visualization_manager.py: å¯è¦–åŒ–

æ¤œè¨¼çµæœ (v026_B_simplifiedæ™‚ç‚¹):
    ãƒ†ã‚¹ãƒˆç²¾åº¦ 78.10% é”æˆ (2025-12-13)
    ã‚³ãƒãƒ³ãƒ‰:
        python3 columnar_ed_ann_v026_multiclass_multilayer_modular_B_simplified.py \\
            --train 3000 --test 3000 --epochs 30 --hidden 512 --lr 0.20 \\
            --u1 0.5 --lateral_lr 0.08 --participation_rate 0.71 --seed 42

TensorFlowãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æ¤œè¨¼ (v027.2):
    ã€NumPy vs TensorFlow æ¯”è¼ƒå®Ÿé¨“ã€‘(70ã‚¨ãƒãƒƒã‚¯ã€2025-12-20)
    - å­¦ç¿’A (NumPy): Best 82.90%, Final 75.60%, SD 8.24%
    - å­¦ç¿’B (TensorFlow): Best 82.40%, Final 75.60%, SD 5.31%
    - çµè«–: ç²¾åº¦åŒç­‰ã€å®‰å®šæ€§35.6%å‘ä¸Š â†’ NumPyå‰Šé™¤ãƒ»TensorFlowä¸€æœ¬åŒ–ã‚’æ±ºå®š
    
    ã€4ãƒ¢ãƒ¼ãƒ‰å‹•ä½œæ¤œè¨¼ã€‘(v027.2æœ€çµ‚ç‰ˆã€2025-12-20)
    - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰: âœ“ PASS
    - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šï¼‰: âœ“ PASS  
    - ãƒŸãƒ‹ãƒãƒƒãƒï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰: âœ“ PASS
    - ãƒŸãƒ‹ãƒãƒƒãƒï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Šï¼‰: âœ“ PASS
    
    è©³ç´°: TENSORFLOW_DATALOADER_GUIDE.md å‚ç…§
"""

import os
# TensorFlowã®ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ‘åˆ¶ï¼ˆæƒ…å ±ãƒ»è­¦å‘Šã‚’éè¡¨ç¤ºï¼‰
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import argparse
import numpy as np
import time
from collections import defaultdict

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules.hyperparameters import HyperParams
from modules.data_loader import (
    load_dataset, load_custom_dataset, resolve_dataset_path,
    create_tf_dataset, get_class_names
)
from modules.ed_network import RefinedDistributionEDNetwork
from modules.visualization_manager import VisualizationManager
from epoch_monitor import DetailedEpochMonitor


def parse_weight_init_scales(wis_str):
    """
    é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã®æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹
    
    è¨˜æ³•:
        "2.25, 2.75, 12.00"           -> [2.25, 2.75, 12.00]
        "2.25, 3.0[9], 12.00"         -> [2.25, 3.0, 3.0, ..., 3.0, 12.00] (3.0ãŒ9å€‹)
        "2.0, 3.0[5], 3.5[3], 12.00"  -> [2.0, 3.0Ã—5, 3.5Ã—3, 12.00]
    
    Args:
        wis_str: é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã®æ–‡å­—åˆ—
    
    Returns:
        list[float]: ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®ä¿‚æ•°ãƒªã‚¹ãƒˆ
    
    Raises:
        ValueError: ä¸æ­£ãªå½¢å¼ã®å ´åˆ
    """
    import re
    
    scales = []
    items = [item.strip() for item in wis_str.split(',')]
    
    for item in items:
        # ç¹°ã‚Šè¿”ã—è¨˜æ³•ã®ãƒã‚§ãƒƒã‚¯: "3.0[10]"
        match = re.match(r'^([\d.]+)\[(\d+)\]$', item)
        if match:
            value = float(match.group(1))
            count = int(match.group(2))
            if count <= 0:
                raise ValueError(f"ç¹°ã‚Šè¿”ã—å›æ•°ã¯1ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {item}")
            scales.extend([value] * count)
        else:
            # é€šå¸¸ã®å€¤
            try:
                scales.append(float(item))
            except ValueError:
                raise ValueError(f"ä¸æ­£ãªå€¤ã®å½¢å¼ã§ã™: '{item}'\n"
                               f"æ­£ã—ã„å½¢å¼: '2.25' ã¾ãŸã¯ '3.0[10]' (å€¤[ç¹°ã‚Šè¿”ã—å›æ•°])")
    
    return scales


def analyze_affinity_distribution(network):
    """
    Affinityåˆ†å¸ƒã‚’è§£æã™ã‚‹
    
    Args:
        network: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    
    Returns:
        dict: Affinityåˆ†å¸ƒæƒ…å ±
    """
    print("\n" + "=" * 70)
    print("Affinityåˆ†å¸ƒè§£æ")
    print("=" * 70)
    
    for layer_idx, affinity in enumerate(network.column_affinity_all_layers):
        print(f"\n[Layer {layer_idx + 1}] å½¢çŠ¶: {affinity.shape}")
        
        # å…¨ä½“çµ±è¨ˆ
        print(f"  å…¨ä½“çµ±è¨ˆ:")
        print(f"    æœ€å°å€¤: {np.min(affinity):.6e}")
        print(f"    æœ€å¤§å€¤: {np.max(affinity):.6e}")
        print(f"    å¹³å‡å€¤: {np.mean(affinity):.6e}")
        print(f"    ä¸­å¤®å€¤: {np.median(affinity):.6e}")
        print(f"    æ¨™æº–åå·®: {np.std(affinity):.6e}")
        
        # éã‚¼ãƒ­è¦ç´ ã®çµ±è¨ˆ
        non_zero_affinity = affinity[affinity > 0]
        if len(non_zero_affinity) > 0:
            print(f"  éã‚¼ãƒ­è¦ç´  ({len(non_zero_affinity)}å€‹):")
            print(f"    æœ€å°å€¤: {np.min(non_zero_affinity):.6e}")
            print(f"    æœ€å¤§å€¤: {np.max(non_zero_affinity):.6e}")
            print(f"    å¹³å‡å€¤: {np.mean(non_zero_affinity):.6e}")
        
        # é–¾å€¤åˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚«ã‚¦ãƒ³ãƒˆ
        thresholds = [0.0, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 0.1, 0.5]
        print(f"  é–¾å€¤åˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆå„ã‚¯ãƒ©ã‚¹ã®å¹³å‡ï¼‰:")
        for i in range(len(thresholds) - 1):
            low, high = thresholds[i], thresholds[i+1]
            count_per_class = []
            for class_idx in range(affinity.shape[0]):
                count = np.sum((affinity[class_idx] > low) & (affinity[class_idx] <= high))
                count_per_class.append(count)
            avg_count = np.mean(count_per_class)
            print(f"    {low:.0e} < affinity â‰¤ {high:.0e}: {avg_count:.1f}å€‹/ã‚¯ãƒ©ã‚¹")
        
        # é«˜affinity (> 1e-8) ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
        high_affinity_count = np.sum(np.max(affinity, axis=0) > 1e-8)
        total_neurons = affinity.shape[1]
        print(f"  é«˜affinityï¼ˆ>1e-8ï¼‰ã‚’æŒã¤ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³: {high_affinity_count}/{total_neurons} ({high_affinity_count/total_neurons*100:.1f}%)")
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã®éã‚¼ãƒ­ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°
        print(f"  ã‚¯ãƒ©ã‚¹åˆ¥ã®éã‚¼ãƒ­affinityæŒã¡ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°:")
        for class_idx in range(min(affinity.shape[0], 10)):  # æœ€å¤§10ã‚¯ãƒ©ã‚¹è¡¨ç¤º
            count = np.sum(affinity[class_idx] > 0)
            print(f"    ã‚¯ãƒ©ã‚¹{class_idx}: {count}å€‹")
    
    print("=" * 70)


def analyze_dead_neurons(network, x_data, y_data, epoch_label=""):
    """
    ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’è§£æã™ã‚‹
    
    Args:
        network: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        x_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        y_data: ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
        epoch_label: ã‚¨ãƒãƒƒã‚¯ãƒ©ãƒ™ãƒ«ï¼ˆè¡¨ç¤ºç”¨ï¼‰
    
    Returns:
        dict: ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æƒ…å ±
            - 'dead_counts': å„å±¤ã®ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã®ãƒªã‚¹ãƒˆ
            - 'total_counts': å„å±¤ã®ç·ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã®ãƒªã‚¹ãƒˆ
            - 'dead_ratios': å„å±¤ã®ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ç‡ã®ãƒªã‚¹ãƒˆ
            - 'summary': ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—
    """
    from collections import defaultdict
    
    # å„å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ´»æ€§åŒ–ã‚«ã‚¦ãƒ³ãƒˆ
    activation_counts = [defaultdict(int) for _ in range(len(network.n_hidden))]
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã§forwardå®Ÿè¡Œ
    for x, y in zip(x_data, y_data):
        z_hiddens, z_output, _ = network.forward(x)
        
        # å„å±¤ã®å‹è€…ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’è¨˜éŒ²
        for layer_idx, z_hidden in enumerate(z_hiddens):
            # k-winner selectionï¼ˆä¸Šä½kå€‹ã‚’é¸æŠï¼‰
            if hasattr(network, 'k_winners') and network.k_winners > 0:
                k = min(network.k_winners, len(z_hidden))
                top_k_indices = np.argsort(z_hidden)[-k:]
                for idx in top_k_indices:
                    activation_counts[layer_idx][idx] += 1
            else:
                # k-winneræœªæŒ‡å®šã®å ´åˆã¯ã€æ­£ã®æ´»æ€§åŒ–ã‚’æŒã¤å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
                for idx, activation in enumerate(z_hidden):
                    if activation > 0:
                        activation_counts[layer_idx][idx] += 1
    
    # ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚«ã‚¦ãƒ³ãƒˆ
    dead_counts = []
    total_counts = []
    dead_ratios = []
    
    for layer_idx, n_neurons in enumerate(network.n_hidden):
        active_neurons = len(activation_counts[layer_idx])
        dead_neurons = n_neurons - active_neurons
        dead_ratio = dead_neurons / n_neurons if n_neurons > 0 else 0.0
        
        dead_counts.append(dead_neurons)
        total_counts.append(n_neurons)
        dead_ratios.append(dead_ratio)
    
    # ã‚µãƒãƒªãƒ¼æ–‡å­—åˆ—ã®ç”Ÿæˆ
    summary_parts = []
    for layer_idx, (dead, total, ratio) in enumerate(zip(dead_counts, total_counts, dead_ratios)):
        summary_parts.append(f"L{layer_idx+1}:{dead}/{total}")
    
    summary = f"Dead={', '.join(summary_parts)}"
    
    return {
        'dead_counts': dead_counts,
        'total_counts': total_counts,
        'dead_ratios': dead_ratios,
        'summary': summary,
        'epoch_label': epoch_label
    }


def parse_args():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ"""
    parser = argparse.ArgumentParser(
        description='ã‚³ãƒ©ãƒ EDæ³• å¤šå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®Ÿè£… (ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ç‰ˆ)\n'
                    '\n'
                    'ã€é‡è¦ã€‘å±¤æ•°ã«åŸºã¥ãè‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š:\n'
                    '  - 1å±¤ã‹ã‚‰5å±¤ã¾ã§ã®æ§‹æˆã«ã¯å†…éƒ¨ãƒ†ãƒ¼ãƒ–ãƒ«ã§ä¿æŒã—ã¦ã„ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè‡ªå‹•é©ç”¨ã•ã‚Œã¾ã™\n'
                    '  - 6å±¤ä»¥ä¸Šã®æ§‹æˆã«ã¯5å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦é©ç”¨ã•ã‚Œã¾ã™\n'
                    '  - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æ˜ç¤ºçš„ã«æŒ‡å®šã—ãŸå€¤ã¯è‡ªå‹•è¨­å®šã‚ˆã‚Šå„ªå…ˆã•ã‚Œã¾ã™\n'
                    '  - --list_hyperparams ã§åˆ©ç”¨å¯èƒ½ãªè¨­å®šä¸€è¦§ã‚’ç¢ºèªã§ãã¾ã™\n',
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    # ========================================
    # å®Ÿè¡Œé–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    # ========================================
    exec_group = parser.add_argument_group('å®Ÿè¡Œé–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
    exec_group.add_argument('--train', type=int, default=3000,
                           help='è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 3000ï¼‰')
    exec_group.add_argument('--test', type=int, default=1000,
                           help='ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 1000ï¼‰')
    exec_group.add_argument('--epochs', type=int, default=None,
                           help='ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: å±¤æ•°ã«å¿œã˜ãŸè‡ªå‹•è¨­å®šï¼‰')
    exec_group.add_argument('--seed', type=int, default=42,
                           help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 42ã€å†ç¾æ€§ç¢ºä¿ç”¨ï¼‰')
    exec_group.add_argument('--fashion', action='store_true',
                           help='Fashion-MNISTã‚’ä½¿ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹å­˜ã€--dataset fashionã‚’æ¨å¥¨ï¼‰')
    exec_group.add_argument('--dataset', type=str, default=None,
                           help='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆmnist, fashion, cifar10, cifar100ï¼‰ã¾ãŸã¯ '
                                'ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã€‚'
                                'ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã¯~/.keras/datasets/é…ä¸‹ã«é…ç½®ã™ã‚‹ã‹ã€çµ¶å¯¾ãƒ‘ã‚¹ã§æŒ‡å®šã€‚'
                                'è©³ç´°ã¯CUSTOM_DATASET_GUIDE.mdã‚’å‚ç…§ã€‚')
    
    # ========================================
    # EDæ³•é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    # ========================================
    ed_group = parser.add_argument_group('EDæ³•é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
    ed_group.add_argument('--hidden', type=str, default='512',
                         help='éš ã‚Œå±¤ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆä¾‹: 512=1å±¤, 256,128=2å±¤ï¼‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 512ï¼‰')
    ed_group.add_argument('--activation', type=str, default='tanh',
                         choices=['tanh', 'sigmoid', 'leaky_relu', 'clipped_leaky_relu', 'shifted_sigmoid', 'clipped_identity'],
                         help='æ´»æ€§åŒ–é–¢æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: tanhï¼‰â€»ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒç”¨ã€å°†æ¥çš„ã«å‰Šé™¤äºˆå®š')
    ed_group.add_argument('--leaky-alpha', type=float, default=0.1,
                         help='Leaky ReLUã®è² å‹¾é…ä¿‚æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.1ã€æ¨å¥¨å€¤: 0.01-0.2ï¼‰')
    ed_group.add_argument('--lr', type=float, default=None,
                         help='å­¦ç¿’ç‡ï¼ˆå±¤æ•°ã«ã‚ˆã‚Šè‡ªå‹•è¨­å®š: 1å±¤=0.20, 2å±¤=0.25ã€æ˜ç¤ºæŒ‡å®šã§ä¸Šæ›¸ãï¼‰')
    ed_group.add_argument('--u1', type=float, default=None,
                         help='ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•°u1ï¼ˆå‡ºåŠ›å±¤â†’æœ€çµ‚éš ã‚Œå±¤ã€å±¤æ•°ã«ã‚ˆã‚Šè‡ªå‹•è¨­å®š: 1å±¤=0.5, 2å±¤=0.5ã€æ˜ç¤ºæŒ‡å®šã§ä¸Šæ›¸ãï¼‰')
    ed_group.add_argument('--u2', type=float, default=None,
                         help='ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•°u2ï¼ˆéš ã‚Œå±¤é–“ã€å±¤æ•°ã«ã‚ˆã‚Šè‡ªå‹•è¨­å®š: 1å±¤=0.8, 2å±¤=0.8ã€æ˜ç¤ºæŒ‡å®šã§ä¸Šæ›¸ãï¼‰')
    ed_group.add_argument('--lateral_lr', type=float, default=None,
                         help='å´æ–¹æŠ‘åˆ¶ã®å­¦ç¿’ç‡ï¼ˆå±¤æ•°ã«ã‚ˆã‚Šè‡ªå‹•è¨­å®š: 1å±¤=0.08, 2å±¤=0.08ã€æ˜ç¤ºæŒ‡å®šã§ä¸Šæ›¸ãï¼‰')
    ed_group.add_argument('--gradient_clip', type=float, default=0.05,
                         help='gradient clippingå€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 0.05ï¼‰')
    ed_group.add_argument('--batch', type=int, default=None,
                         help='ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆæœªæŒ‡å®š=ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã€32/128æ¨å¥¨ï¼‰')
    ed_group.add_argument('--shuffle', action='store_true',
                         help='ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆTensorFlow Dataset APIä½¿ç”¨ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³/ãƒŸãƒ‹ãƒãƒƒãƒä¸¡å¯¾å¿œï¼‰')
    ed_group.add_argument('--use_ridge', action='store_true',
                         help='â˜…æœ€é©åŒ–â˜… Ridgeå›å¸°ã‚’ä½¿ç”¨ï¼ˆELM/RCæ¨™æº–æ‰‹æ³•ã€å­¦ç¿’é«˜é€ŸåŒ–ï¼‰')
    ed_group.add_argument('--ridge_lambda', type=float, default=1e-3,
                         help='Ridgeå›å¸°ã®æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-3ï¼‰')
    ed_group.add_argument('--weight_sparsity', type=float, default=0.0,
                         help='â˜…æœ€é©åŒ–â˜… ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒªã‚¶ãƒ¼ãƒï¼ˆ0.0ã€œ1.0ã€0.1=90%%ã‚¹ãƒ‘ãƒ¼ã‚¹ã€RC/ELMæ¨™æº–æ‰‹æ³•ï¼‰')
    
    # ========================================
    # ã‚³ãƒ©ãƒ é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    # ========================================
    column_group = parser.add_argument_group('ã‚³ãƒ©ãƒ é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
    column_group.add_argument('--list_hyperparams', action='store_true',
                             help='åˆ©ç”¨å¯èƒ½ãªHyperParamsè¨­å®šä¸€è¦§ã‚’è¡¨ç¤ºã—ã¦çµ‚äº†')
    column_group.add_argument('--column_radius', type=float, default=None,
                             help='ã‚³ãƒ©ãƒ åŠå¾„ï¼ˆå±¤æ•°ã«ã‚ˆã‚Šè‡ªå‹•è¨­å®šã€æ˜ç¤ºæŒ‡å®šã§ä¸Šæ›¸ãï¼‰')
    column_group.add_argument('--participation_rate', type=float, default=None,
                             help='ã‚³ãƒ©ãƒ å‚åŠ ç‡ï¼ˆå±¤æ•°ã«ã‚ˆã‚Šè‡ªå‹•è¨­å®šã€æ˜ç¤ºæŒ‡å®šã§ä¸Šæ›¸ãï¼‰')
    column_group.add_argument('--column_neurons', type=int, default=None,
                             help='å„ã‚¯ãƒ©ã‚¹ã®æ˜ç¤ºçš„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: Noneã€é‡è¤‡è¨±å®¹ã€å„ªå…ˆåº¦ï¼šä¸­ï¼‰')
    column_group.add_argument('--use_circular', action='store_true',
                             help='æ—§å††ç’°æ§‹é€ ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒãƒ‹ã‚«ãƒ ï¼‰')
    column_group.add_argument('--overlap', type=float, default=0.0,
                             help='ã‚³ãƒ©ãƒ é–“ã®é‡è¤‡åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: 0.0ã€0.0-1.0ã€å††ç’°æ§‹é€ ã§ã®ã¿æœ‰åŠ¹ã€0.0=é‡è¤‡ãªã—ï¼‰')
    column_group.add_argument('--diagnose_column', action='store_true',
                             help='ã‚³ãƒ©ãƒ æ§‹é€ ã®è©³ç´°è¨ºæ–­ã‚’å®Ÿè¡Œ')
    column_group.add_argument('--wis', '--weight_init_scales', type=str, default=None,
                             dest='weight_init_scales',
                             help='é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã€‚éš ã‚Œå±¤ã¨å‡ºåŠ›å±¤åˆ†ã‚’åˆã‚ã›ã¦æŒ‡å®šã™ã‚‹ã€‚\n'
                                  'ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ç¹°ã‚Šè¿”ã—ã¯å€¤[å›æ•°]ã§æŒ‡å®šã€‚\n'
                                  'ä¾‹: 2.25,2.75,12.00 (éš ã‚Œå±¤2å±¤ã¨å‡ºåŠ›å±¤)\n'
                                  '    2.25,3.0[9],12.00 (éš ã‚Œå±¤10å±¤ã¨å‡ºåŠ›å±¤ã€‚Layer1-9ã‚’3.0ã«è¨­å®š)\n'
                                  '    2.0,3.0[99],12.00 (éš ã‚Œå±¤100å±¤ã¨å‡ºåŠ›å±¤)\n'
                                  'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—')
    
    # ========================================
    # å¯è¦–åŒ–é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    # ========================================
    viz_group = parser.add_argument_group('å¯è¦–åŒ–é–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
    viz_group.add_argument('--viz', action='store_true',
                          help='å­¦ç¿’æ›²ç·šã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã‚’æœ‰åŠ¹åŒ–')
    viz_group.add_argument('--heatmap', action='store_true',
                          help='æ´»æ€§åŒ–ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è¡¨ç¤ºã‚’æœ‰åŠ¹åŒ–ï¼ˆ--vizã¨ä½µç”¨ï¼‰')
    viz_group.add_argument('--save_viz', type=str, nargs='?', const='viz_results/',
                          default=None, metavar='PATH',
                          help='å¯è¦–åŒ–çµæœã‚’ä¿å­˜ã€‚'
                               'ãƒ‘ã‚¹æŒ‡å®š: æœ«å°¾"/"ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰ã€æœ«å°¾"/"ãªã—ã§ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã€‚'
                               'ä¾‹: results/ â†’ results/viz_results_20251221_153045.pngã€'
                               'results/exp1 â†’ results/exp1.pngã€‚'
                               'å­¦ç¿’æ›²ç·šã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’åŒæ™‚ã«ä¿å­˜ã™ã‚‹å ´åˆã¯ã€_viz.pngã¨_heatmap.pngãŒä»˜åŠ ã•ã‚Œã¾ã™ã€‚'
                               'å¼•æ•°ãªã—: viz_results/ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã§ä¿å­˜ã€‚'
                               'ã‚ªãƒ—ã‚·ãƒ§ãƒ³æœªæŒ‡å®š: ä¿å­˜ã—ãªã„ã€‚')
    
    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    args = parse_args()
    
    # ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
    if args.seed is not None:
        print(f"\n=== ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š: {args.seed} ===")
        import random
        random.seed(args.seed)
        np.random.seed(args.seed)
        print("å†ç¾æ€§ãƒ¢ãƒ¼ãƒ‰: æœ‰åŠ¹ï¼ˆrandom, numpyå›ºå®šï¼‰\n")
    else:
        print("\n=== ä¹±æ•°ã‚·ãƒ¼ãƒ‰: ãƒ©ãƒ³ãƒ€ãƒ  ===")
        print("å†ç¾æ€§ãƒ¢ãƒ¼ãƒ‰: ç„¡åŠ¹ï¼ˆæ¯å›ç•°ãªã‚‹çµæœï¼‰\n")
    
    # HyperParamsè¨­å®šä¸€è¦§ã®è¡¨ç¤º
    if args.list_hyperparams:
        hp = HyperParams()
        hp.list_configs()
        import sys
        sys.exit(0)
    
    # ========================================
    # 1. éš ã‚Œå±¤ã®ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå¯¾å¿œã€å¤šå±¤å¯¾å¿œï¼‰
    # ========================================
    if ',' in args.hidden:
        hidden_sizes = [int(x.strip()) for x in args.hidden.split(',')]
    else:
        hidden_sizes = [int(args.hidden)]
    
    # ========================================
    # 2. HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã®è‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
    # ========================================
    hp = HyperParams()
    n_layers = len(hidden_sizes)
    
    try:
        config = hp.get_config(n_layers)
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æ˜ç¤ºã•ã‚Œã¦ã„ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆNoneï¼‰ã®ã¿HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã®å€¤ã§ä¸Šæ›¸ã
        # hidden_sizesã¯å¸¸ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã®å€¤ã‚’ä½¿ç”¨ï¼ˆå±¤æ•°ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸã‚‚ã®ã‚’å°Šé‡ï¼‰
        if args.hidden == '512':  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®å ´åˆã®ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹æˆã‚’ä½¿ç”¨
            hidden_sizes = config['hidden']
        
        # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Noneã®å ´åˆã®ã¿ãƒ†ãƒ¼ãƒ–ãƒ«å€¤ã‚’é©ç”¨ï¼ˆæ˜ç¤ºæŒ‡å®šã•ã‚ŒãŸå€¤ã¯å°Šé‡ï¼‰
        if args.lr is None:
            args.lr = config['learning_rate']
        if args.u1 is None and 'u1' in config:
            args.u1 = config['u1']
        if args.u2 is None and 'u2' in config:
            args.u2 = config['u2']
        if args.lateral_lr is None and 'lateral_lr' in config:
            args.lateral_lr = config['lateral_lr']
        if args.column_radius is None:
            args.column_radius = config['column_radius']
        if args.participation_rate is None and 'participation_rate' in config:
            args.participation_rate = config['participation_rate']
        if args.epochs is None:
            args.epochs = config['epochs']
        
        # é©ç”¨å¾Œã®å®Ÿéš›ã®å€¤ã‚’è¡¨ç¤º
        print(f"\n=== å±¤æ•°ã«åŸºã¥ãHyperParamsè¨­å®šã‚’è‡ªå‹•é©ç”¨ï¼ˆ{n_layers}å±¤ï¼‰ ===")
        print("*** ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æ˜ç¤ºçš„ã«æŒ‡å®šã•ã‚ŒãŸå€¤ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«å€¤ã‚ˆã‚Šå„ªå…ˆã•ã‚Œã¾ã™ã€‚")
        print(f"hidden_layers: {hidden_sizes}")
        print(f"learning_rate: {args.lr}")
        print(f"u1: {args.u1}")
        print(f"u2: {args.u2}")
        print(f"lateral_lr: {args.lateral_lr}")
        print(f"column_radius: {args.column_radius}")
        print(f"participation_rate: {args.participation_rate}")
        print(f"epochs: {args.epochs}")
        
        print("="*70 + "\n")
    except ValueError as e:
        print(f"Warning: HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã®å–å¾—ã«å¤±æ•—: {e}")
        print("å€‹åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç¶™ç¶šã—ã¾ã™ã€‚\n")
    
    # ========================================
    # 2.5 é‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã®è§£æ±º
    # ========================================
    weight_init_scales = None
    weight_init_source = None
    
    if args.weight_init_scales:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå ´åˆ
        try:
            weight_init_scales = parse_weight_init_scales(args.weight_init_scales)
            
            # å±¤æ•°ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            expected_count = n_layers + 1  # éš ã‚Œå±¤ + å‡ºåŠ›å±¤
            actual_count = len(weight_init_scales)
            if actual_count != expected_count:
                print("\n" + "="*70)
                print("ã‚¨ãƒ©ãƒ¼: --hiddenã¨--wisã®å±¤æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
                print("="*70)
                print(f"--hiddenæŒ‡å®š: {args.hidden}")
                print(f"  â†’ éš ã‚Œå±¤æ•°: {n_layers}å±¤ + å‡ºåŠ›å±¤ = åˆè¨ˆ{expected_count}å€‹ã®ä¿‚æ•°ãŒå¿…è¦")
                print(f"\n--wisæŒ‡å®š: {args.weight_init_scales}")
                print(f"  â†’ ãƒ‘ãƒ¼ã‚¹çµæœ: {weight_init_scales}")
                print(f"  â†’ ä¿‚æ•°ã®å€‹æ•°: {actual_count}å€‹")
                print(f"\nä¸è¶³/éå‰°: {actual_count - expected_count:+d}å€‹")
                print("="*70 + "\n")
                raise ValueError(
                    f"å±¤æ•°ä¸ä¸€è‡´: {expected_count}å€‹ã®ä¿‚æ•°ãŒå¿…è¦ã§ã™ãŒã€{actual_count}å€‹æŒ‡å®šã•ã‚Œã¾ã—ãŸ"
                )
            
            weight_init_source = "CLI"
            print(f"[é‡ã¿åˆæœŸåŒ–ä¿‚æ•°] ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰å–å¾—: {weight_init_scales}")
        except ValueError as e:
            print(f"ã‚¨ãƒ©ãƒ¼: --wiså¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã®å€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")
            args.weight_init_scales = None  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯HyperParamsä½¿ç”¨
    
    if weight_init_scales is None:
        # HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
        try:
            config = hp.get_config(n_layers)
            weight_init_scales = config.get('weight_init_scales', None)
            if weight_init_scales:
                weight_init_source = "HyperParams"
                print(f"[é‡ã¿åˆæœŸåŒ–ä¿‚æ•°] HyperParamsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—: {weight_init_scales}")
            else:
                weight_init_source = "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤"
                print(f"[é‡ã¿åˆæœŸåŒ–ä¿‚æ•°] HyperParamsã«æœªè¨­å®šã®ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
        except:
            weight_init_source = "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤"
            print(f"[é‡ã¿åˆæœŸåŒ–ä¿‚æ•°] HyperParamså–å¾—å¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
    
    # ========================================
    # 3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    # ========================================
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã®è§£æ±ºï¼ˆå„ªå…ˆé †ä½: --dataset > --fashion > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    if args.dataset is not None:
        dataset = args.dataset
    elif args.fashion:
        dataset = 'fashion'
    else:
        dataset = 'mnist'
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹ã®è§£æ±ºï¼ˆæ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ or ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ï¼‰
    from modules.data_loader import resolve_dataset_path, load_custom_dataset
    dataset_path, is_custom = resolve_dataset_path(dataset)
    
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­... (è¨“ç·´:{args.train}, ãƒ†ã‚¹ãƒˆ:{args.test}, ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:{dataset})")
    
    # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹æ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã§èª­ã¿è¾¼ã¿æ–¹æ³•ã‚’åˆ‡ã‚Šæ›¿ãˆ
    custom_class_names = None
    custom_input_shape = None
    if is_custom:
        (x_train, y_train), (x_test, y_test), custom_class_names, custom_input_shape = load_custom_dataset(
            dataset_path=dataset_path, train_samples=args.train, test_samples=args.test
        )
    else:
        (x_train, y_train), (x_test, y_test) = load_dataset(
            dataset=dataset_path, train_samples=args.train, test_samples=args.test
        )
    
    # å…¥åŠ›æ¬¡å…ƒã¨ã‚¯ãƒ©ã‚¹æ•°ã‚’è‡ªå‹•æ¤œå‡º
    n_input = x_train.shape[1]  # è‡ªå‹•æ¤œå‡º: 784 (MNIST/Fashion), 3072 (CIFAR-10), etc.
    n_classes = len(np.unique(y_train))  # è‡ªå‹•æ¤œå‡º: 10, 100, etc.
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±: å…¥åŠ›æ¬¡å…ƒ={n_input}, ã‚¯ãƒ©ã‚¹æ•°={n_classes}")
    
    # ã‚¯ãƒ©ã‚¹åã®å–å¾—ï¼ˆæ¨™æº–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ or ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
    from modules.data_loader import get_class_names
    class_names = get_class_names(dataset, custom_class_names=custom_class_names)
    if class_names:
        print(f"ã‚¯ãƒ©ã‚¹å: {class_names}")
    
    # ========================================
    # 4. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰
    # ========================================
    print("\nãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–ä¸­...")
    
    network = RefinedDistributionEDNetwork(
        n_input=n_input,
        n_hidden=hidden_sizes,
        n_output=n_classes,
        learning_rate=args.lr,
        lateral_lr=args.lateral_lr,
        u1=args.u1,
        u2=args.u2,
        column_radius=args.column_radius,
        column_neurons=args.column_neurons,
        participation_rate=args.participation_rate,
        use_hexagonal=not args.use_circular,
        overlap=args.overlap,
        gradient_clip=args.gradient_clip,
        activation=args.activation,  # activationãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        leaky_alpha=args.leaky_alpha,  # Leaky ReLUã®è² å‹¾é…ä¿‚æ•°
        top_k_winners=3,  # â˜…æ–°æ©Ÿèƒ½â˜… Top-3å”èª¿å­¦ç¿’
        column_weight_diversity=0.0,  # 0.0ãŒæœ€é©ï¼ˆ79.8%ï¼‰ã€é‡ã¿å¤šæ§˜åŒ–ä¸è¦
        use_ridge_regression=args.use_ridge,  # â˜…æœ€é©åŒ–â˜… Ridgeå›å¸°ä½¿ç”¨
        ridge_lambda=args.ridge_lambda,  # â˜…æœ€é©åŒ–â˜… Ridgeå›å¸°æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        weight_sparsity=args.weight_sparsity,  # â˜…æœ€é©åŒ–â˜… ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒªã‚¶ãƒ¼ãƒï¼ˆRC/ELMæ¨™æº–æ‰‹æ³•ï¼‰
        hyperparams=hp,  # HyperParamsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¸¡ã™ï¼ˆé‡ã¿åˆæœŸåŒ–ä¿‚æ•°ã®å–å¾—ã«å¿…è¦ï¼‰
        weight_init_scales=weight_init_scales,  # CLI/HyperParamsã‹ã‚‰å–å¾—ã—ãŸå€¤
        weight_init_source=weight_init_source  # å€¤ã®å‡ºå‡¦ã‚’è¨˜éŒ²
    )
    
    # ã‚³ãƒ©ãƒ æ§‹é€ ã®è¨ºæ–­ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if args.diagnose_column:
        network.diagnose_column_structure()
        print("\nè¨ºæ–­å®Œäº†ã€‚å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        import sys
        sys.exit(0)
    
    # ========================================
    # 5. å¯è¦–åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    # ========================================
    viz_manager = None
    if args.viz:
        try:
            viz_manager = VisualizationManager(
                enable_viz=True,
                enable_heatmap=args.heatmap,
                save_path=args.save_viz,
                total_epochs=args.epochs,
                input_shape=custom_input_shape  # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»åƒå½¢çŠ¶ã‚’æ¸¡ã™
            )
            print("\nå¯è¦–åŒ–æ©Ÿèƒ½: æœ‰åŠ¹")
            if args.heatmap:
                print("  - ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º: æœ‰åŠ¹")
            if args.save_viz:
                print(f"  - ä¿å­˜å…ˆ: {args.save_viz}")
            if custom_input_shape:
                print(f"  - ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ›å½¢çŠ¶: {custom_input_shape}")
        except Exception as e:
            print(f"\nè­¦å‘Š: å¯è¦–åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("å¯è¦–åŒ–ãªã—ã§å­¦ç¿’ã‚’ç¶™ç¶šã—ã¾ã™ã€‚")
            viz_manager = None
    
    # ========================================
    # 6. å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    # ========================================
    print("\n" + "=" * 70)
    print("å­¦ç¿’é–‹å§‹")
    print("=" * 70)
    
    from tqdm import tqdm
    from modules.data_loader import create_tf_dataset
    
    # TensorFlow Datasetä½œæˆï¼ˆæ¡ä»¶ã«å¿œã˜ã¦ï¼‰
    train_dataset_tf = None
    if args.batch is not None:
        # ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’
        shuffle_status = "ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Š" if args.shuffle else "ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—"
        print(f"ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: batch={args.batch}, {shuffle_status}, seed={args.seed}")
        train_dataset_tf = create_tf_dataset(
            x_train, y_train,
            batch_size=args.batch,
            shuffle=args.shuffle,
            seed=args.seed
        )
    elif args.shuffle:
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ + ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆbatch_size=1ã®TensorFlow Datasetï¼‰
        print(f"ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚ã‚Š, seed={args.seed}")
        train_dataset_tf = create_tf_dataset(
            x_train, y_train,
            batch_size=1,
            shuffle=True,
            seed=args.seed
        )
    else:
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰
        print(f"ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰: ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—, seed={args.seed}")
    
    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0
    best_test_acc = 0.0
    best_epoch = 0
    train_acc_history = []
    test_acc_history = []
    
    # ========================================
    # åˆæœŸåŒ–ç›´å¾Œã®ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³èª¿æŸ»
    # ========================================
    print("\n" + "=" * 70)
    print("åˆæœŸåŒ–ç›´å¾Œã®ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³èª¿æŸ»")
    print("=" * 70)
    initial_dead_info = analyze_dead_neurons(network, x_test[:1000], y_test[:1000], epoch_label="åˆæœŸåŒ–ç›´å¾Œ")
    print(f"{initial_dead_info['summary']}")
    for layer_idx, (dead, total, ratio) in enumerate(zip(
        initial_dead_info['dead_counts'], 
        initial_dead_info['total_counts'], 
        initial_dead_info['dead_ratios']
    )):
        print(f"  Layer {layer_idx+1}: {dead}/{total} ({ratio*100:.1f}% dead)")
    print("=" * 70)
    
    # ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¥æ­´ï¼ˆã‚¨ãƒãƒƒã‚¯æ¯ã«è¨˜éŒ²ï¼‰
    dead_neuron_history = [initial_dead_info]
    
    # ========================================
    # è©³ç´°ã‚¨ãƒãƒƒã‚¯ãƒ¢ãƒ‹ã‚¿åˆæœŸåŒ–ï¼ˆEpoch 1-5ã‚’ç›£è¦–ï¼‰
    # ========================================
    epoch_monitor = DetailedEpochMonitor(
        n_classes=n_classes,
        monitor_epochs=[1, 2, 3, 4, 5]
    )
    print("\n" + "=" * 70)
    print("ã‚¨ãƒãƒƒã‚¯è©³ç´°ãƒ¢ãƒ‹ã‚¿: æœ‰åŠ¹ï¼ˆEpoch 1-5ã‚’ç›£è¦–ï¼‰")
    print("=" * 70)
    
    # ========================================
    # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¬ãƒ™ãƒ«ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æœ‰åŠ¹åŒ–ï¼ˆEpoch 1-5ã§ç›£è¦–ï¼‰
    # ========================================
    network.set_neuron_tracking(True)
    print("\n" + "=" * 70)
    print("ã‚³ãƒ©ãƒ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°: æœ‰åŠ¹ï¼ˆEpoch 1-5ã‚’ç›£è¦–ï¼‰")
    print("=" * 70)
    
    # tqdmã‚’ä½¿ã£ãŸã‚¨ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—
    pbar = tqdm(range(1, args.epochs + 1), desc="Training", ncols=120)
    for epoch in pbar:
        epoch_start = time.time()
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³çµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆã‚¨ãƒãƒƒã‚¯é–‹å§‹æ™‚ï¼‰
        if epoch in [1, 2, 3, 4, 5]:
            network.reset_neuron_stats()
        
        # è¨“ç·´
        if network.use_ridge_regression and epoch == 1:
            # â˜…Ridgeå›å¸°: 1å›ã ã‘è§£æçš„ã«æœ€é©åŒ–ï¼ˆã‚¨ãƒãƒƒã‚¯ä¸è¦ï¼‰
            train_loss, train_acc = network.train_readout_ridge(x_train, y_train)
            print(f"\nâ˜…Ridgeå›å¸°å®Œäº†: Train Loss={train_loss:.6f}, Train Acc={train_acc:.4f}")
        elif network.use_ridge_regression and epoch > 1:
            # Ridgeå›å¸°å¾Œã¯å­¦ç¿’ä¸è¦ã€è©•ä¾¡ã®ã¿
            train_loss = 0.0
            train_acc = 0.0
            for i, (x, y_true) in enumerate(zip(x_train, y_train)):
                z_hiddens, z_output, _ = network.forward(x)
                y_pred = np.argmax(z_output)
                train_acc += (y_pred == y_true)
                train_loss += -np.log(z_output[y_true] + 1e-10)
            train_acc /= len(x_train)
            train_loss /= len(x_train)
        elif train_dataset_tf is not None:
            # TensorFlow Dataset APIä½¿ç”¨ï¼ˆãƒŸãƒ‹ãƒãƒƒãƒã¾ãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³+ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼‰
            train_acc, train_loss = network.train_epoch_minibatch_tf(
                train_dataset_tf, x_train, y_train
            )
        else:
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ãªã—ï¼‰
            train_acc, train_loss = network.train_epoch(x_train, y_train)
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è¨˜éŒ²
        if epoch_monitor.should_monitor(epoch):
            for i, (x, y_true) in enumerate(zip(x_train, y_train)):
                z_hiddens, z_output, _ = network.forward(x)
                y_pred = np.argmax(z_output)
                
                # äºˆæ¸¬çµæœã‚’è¨˜éŒ²
                epoch_monitor.record_sample_prediction(
                    epoch=epoch,
                    sample_idx=i,
                    y_true=int(y_true),
                    y_pred=int(y_pred),
                    probs=z_output,
                    is_train=True
                )
                
                # æ´»æ€§åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: æœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
                if i < 100:
                    epoch_monitor.record_sample_activation(
                        epoch=epoch,
                        sample_idx=i,
                        activations=z_hiddens,
                        is_train=True,
                        top_k=10
                    )
        
        # ãƒ†ã‚¹ãƒˆ
        test_acc, test_loss = network.evaluate(x_test, y_test)
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è¨˜éŒ²
        if epoch_monitor.should_monitor(epoch):
            for i, (x, y_true) in enumerate(zip(x_test, y_test)):
                z_hiddens, z_output, _ = network.forward(x)
                y_pred = np.argmax(z_output)
                
                # äºˆæ¸¬çµæœã‚’è¨˜éŒ²
                epoch_monitor.record_sample_prediction(
                    epoch=epoch,
                    sample_idx=i,
                    y_true=int(y_true),
                    y_pred=int(y_pred),
                    probs=z_output,
                    is_train=False
                )
                
                # æ´»æ€§åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¨˜éŒ²ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: æœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
                if i < 100:
                    epoch_monitor.record_sample_activation(
                        epoch=epoch,
                        sample_idx=i,
                        activations=z_hiddens,
                        is_train=False,
                        top_k=10
                    )
            
            # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ã‚’è¨ˆç®—
            epoch_monitor.compute_class_accuracies(epoch)
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ï¼ˆEpoch 1-5ï¼‰
        if epoch in [1, 2, 3, 4, 5]:
            neuron_stats = network.get_neuron_stats()
            report_path = f"column_neuron_behavior_epoch{epoch}.txt"
            epoch_monitor.export_column_neuron_report(
                neuron_stats=neuron_stats,
                column_affinity_all_layers=network.column_membership_all_layers,  # â˜…ä¿®æ­£â˜… Membershipã‚’ä½¿ç”¨
                n_samples=len(x_train),
                filepath=report_path
            )
        
        # ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³èª¿æŸ»ï¼ˆã‚¨ãƒãƒƒã‚¯æ¯ï¼‰
        epoch_dead_info = analyze_dead_neurons(network, x_test[:1000], y_test[:1000], epoch_label=f"Epoch {epoch}")
        dead_neuron_history.append(epoch_dead_info)
        
        # å±¥æ­´è¨˜éŒ²
        train_acc_history.append(train_acc)
        test_acc_history.append(test_acc)
        
        epoch_time = time.time() - epoch_start
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            best_epoch = epoch
        
        # é€²æ—è¡¨ç¤º
        pbar.set_postfix({
            'Train': f'{train_acc:.4f}',
            'Test': f'{test_acc:.4f}',
            'Best': f'{best_test_acc:.4f}',
            'Time': f'{epoch_time:.1f}s'
        })
        
        # è©³ç´°å‡ºåŠ›ï¼ˆã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
        print(f"Epoch {epoch:3d}/{args.epochs}: "
              f"Train={train_acc:.4f} (loss={train_loss:.4f}), "
              f"Test={test_acc:.4f} (loss={test_loss:.4f}), "
              f"{epoch_dead_info['summary']}, "
              f"Time={epoch_time:.2f}s")
        
        # å¯è¦–åŒ–æ›´æ–°
        if viz_manager is not None:
            # æ­£è§£ãƒ©ãƒ™ãƒ«ã®å½¢çŠ¶åˆ¤å®šï¼ˆone-hot â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›ï¼‰
            if len(y_test.shape) > 1 and y_test.shape[1] > 1:
                y_test_indices = np.argmax(y_test, axis=1)
            else:
                y_test_indices = y_test
            
            # å­¦ç¿’æ›²ç·šã®æ›´æ–°
            viz_manager.update_learning_curve(
                train_acc_history,
                test_acc_history,
                x_test,
                y_test_indices,
                network
            )
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æ›´æ–°ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚’1ã¤é¸æŠï¼‰
            if args.heatmap and epoch % 1 == 0:
                sample_idx = np.random.randint(0, len(x_test))
                sample_x = x_test[sample_idx]
                sample_y_true = y_test_indices[sample_idx]
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®forward()ã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºãªäºˆæ¸¬ã‚’å–å¾—
                z_hiddens, z_output, _ = network.forward(sample_x)
                sample_y_pred = np.argmax(z_output)
                
                # ã‚¯ãƒ©ã‚¹åã®å–å¾—
                class_names = get_class_names(dataset)
                
                if class_names is not None:
                    true_class_name = class_names[sample_y_true]
                    pred_class_name = class_names[sample_y_pred]
                else:
                    true_class_name = None
                    pred_class_name = None
                
                viz_manager.update_heatmap(
                    epoch=epoch,
                    sample_x=sample_x,
                    sample_y_true=sample_y_true,
                    sample_y_true_name=true_class_name,
                    z_hiddens=z_hiddens,
                    z_output=z_output,
                    sample_y_pred=sample_y_pred,
                    sample_y_pred_name=pred_class_name
                )
    
    # ========================================
    # 7. çµæœã‚µãƒãƒªãƒ¼
    # ========================================
    print("\n" + "=" * 70)
    print("å­¦ç¿’å®Œäº†")
    print("=" * 70)
    print(f"æœ€çµ‚ç²¾åº¦: Train={train_acc:.4f}, Test={test_acc:.4f}")
    print(f"ãƒ™ã‚¹ãƒˆç²¾åº¦: Test={best_test_acc:.4f} (Epoch {best_epoch})")
    
    # ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³è§£æã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³è§£æ")
    print("=" * 70)
    
    # åˆæœŸåŒ–ç›´å¾Œã¨æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®ãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã‚’è¡¨ç¤º
    if len(dead_neuron_history) > 0:
        initial_info = dead_neuron_history[0]
        final_info = dead_neuron_history[-1]
        
        print(f"\nåˆæœŸåŒ–ç›´å¾Œ: {initial_info['summary']}")
        for layer_idx, (dead, total, ratio) in enumerate(zip(
            initial_info['dead_counts'], 
            initial_info['total_counts'], 
            initial_info['dead_ratios']
        )):
            print(f"  Layer {layer_idx+1}: {dead}/{total} ({ratio*100:.1f}% dead)")
        
        print(f"\næœ€çµ‚ã‚¨ãƒãƒƒã‚¯: {final_info['summary']}")
        for layer_idx, (dead, total, ratio) in enumerate(zip(
            final_info['dead_counts'], 
            final_info['total_counts'], 
            final_info['dead_ratios']
        )):
            print(f"  Layer {layer_idx+1}: {dead}/{total} ({ratio*100:.1f}% dead)")
        
        # æ”¹å–„åº¦ã‚’è¨ˆç®—
        print("\nãƒ‡ãƒƒãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å¤‰åŒ–:")
        for layer_idx in range(len(initial_info['dead_counts'])):
            initial_dead = initial_info['dead_counts'][layer_idx]
            final_dead = final_info['dead_counts'][layer_idx]
            diff = final_dead - initial_dead
            diff_sign = "+" if diff > 0 else ""
            print(f"  Layer {layer_idx+1}: {initial_dead} â†’ {final_dead} ({diff_sign}{diff})")
    
    print("=" * 70)
    
    # ========================================
    # ã‚¨ãƒãƒƒã‚¯è©³ç´°ãƒ¢ãƒ‹ã‚¿ã®åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    # ========================================
    print("\n" + "=" * 70)
    print("ã‚¨ãƒãƒƒã‚¯è©³ç´°åˆ†æ")
    print("=" * 70)
    
    # ã‚µãƒ³ãƒ—ãƒ«é›£æ˜“åº¦ã®åˆ†é¡
    epoch_monitor.classify_sample_difficulty()
    
    # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    report_path = "epoch_monitor_report.txt"
    epoch_monitor.export_summary_report(report_path)
    print(f"ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {report_path}")
    
    # CSVå‡ºåŠ›ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
    csv_train_path = "epoch_monitor_train_details.csv"
    csv_test_path = "epoch_monitor_test_details.csv"
    epoch_monitor.export_detailed_csv(csv_train_path, data_type='train')
    epoch_monitor.export_detailed_csv(csv_test_path, data_type='test')
    print(f"è©³ç´°ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›: {csv_train_path}, {csv_test_path}")
    
    # Testèª¤ã‚Šåˆ†æãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    error_report_path = "test_error_analysis.txt"
    epoch_monitor.export_test_error_report(error_report_path)
    print(f"Testèª¤ã‚Šåˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {error_report_path}")
    
    # Testèª¤ã‚Šé·ç§»åˆ†æãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    transition_report_path = "test_error_transitions.txt"
    epoch_monitor.export_error_transition_report(transition_report_path)
    print(f"Testèª¤ã‚Šé·ç§»åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {transition_report_path}")
    
    # ç°¡æ˜“ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("\nã€ã‚¨ãƒãƒƒã‚¯1â†’2ã®é·ç§»åˆ†æ (Train)ã€‘")
    transition_train_12 = epoch_monitor.analyze_epoch_transitions(1, 2, 'train')
    if transition_train_12:
        print(f"  æ­£è§£â†’æ­£è§£: {transition_train_12['correct_to_correct']['count']}")
        print(f"  æ­£è§£â†’ä¸æ­£è§£: {transition_train_12['correct_to_wrong']['count']}")
        print(f"  ä¸æ­£è§£â†’æ­£è§£: {transition_train_12['wrong_to_correct']['count']} â† æ”¹å–„")
        print(f"  ä¸æ­£è§£â†’ä¸æ­£è§£: {transition_train_12['wrong_to_wrong']['count']}")
    
    print("\nã€ã‚¨ãƒãƒƒã‚¯1â†’2ã®é·ç§»åˆ†æ (Test)ã€‘")
    transition_test_12 = epoch_monitor.analyze_epoch_transitions(1, 2, 'test')
    if transition_test_12:
        print(f"  æ­£è§£â†’æ­£è§£: {transition_test_12['correct_to_correct']['count']}")
        print(f"  æ­£è§£â†’ä¸æ­£è§£: {transition_test_12['correct_to_wrong']['count']}")
        print(f"  ä¸æ­£è§£â†’æ­£è§£: {transition_test_12['wrong_to_correct']['count']} â† æ”¹å–„")
        print(f"  ä¸æ­£è§£â†’ä¸æ­£è§£: {transition_test_12['wrong_to_wrong']['count']}")
    
    print("=" * 70)
    
    # å¯è¦–åŒ–ã®æœ€çµ‚å‡¦ç†
    if viz_manager is not None:
        saved_viz, saved_heatmap = viz_manager.save_figures()
        if saved_viz or saved_heatmap:
            print("\nå¯è¦–åŒ–çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
            if saved_viz:
                print(f"  - å­¦ç¿’æ›²ç·š: {saved_viz}")
            if saved_heatmap:
                print(f"  - ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—: {saved_heatmap}")
    
    print("\n" + "=" * 70)


if __name__ == '__main__':
    main()
