# コラム構造を導入したED法による多クラス分類

**生物学的に妥当な学習アルゴリズムで、脳の構造を模倣した多クラス分類を実現**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## 📌 プロジェクト概要

### 一行説明

金子勇氏のED法（Error Diffusion Learning Algorithm）に大脳皮質のコラム構造を導入し、生物学的妥当性を保ちながら多クラス分類を実現した学習アルゴリズムの実装です。

### 主な特徴

✨ **生物学的妥当性**: 誤差逆伝播を使わない、脳の学習メカニズムに近い学習方式  
🧠 **コラム構造**: 大脳皮質の柱状構造を模倣した、クラスごとの専用ニューロン群  
🎯 **高精度**: MNISTで訓練79.0%、テスト73.2%を達成（3000/1000サンプル、10エポック）  
⚡ **効率的**: 1つの共有重み空間で多クラス分類、メモリ効率が高い  
🔬 **研究指向**: オリジナルED法の理論を100%保持し、拡張可能な設計

### 達成した性能

**MNIST手書き数字認識（10クラス分類）**:
```
訓練データ: 3000サンプル、テストデータ: 1000サンプル、10エポック
訓練精度: 79.0%
テスト精度: 73.2%（最高）、72.6%（最終エポック）
```

**学習の進行**:
| Epoch | 訓練精度 | テスト精度 |
|-------|---------|-----------|
| 1 | 21.4% | 25.9% |
| 3 | 58.4% | 56.6% |
| 5 | 70.6% | 64.3% |
| 7 | 75.3% | 71.5% |
| 9 | 78.2% | **73.2%** |
| 10 | 79.0% | 72.6% |

---

## 🧠 コラム構造を導入したED法とは？

### オリジナルED法の紹介

**ED法（Error Diffusion Learning Algorithm）**は、金子勇氏により1999年に発案された、生物学的に妥当な学習アルゴリズムです。

**従来の機械学習との3つの違い**:

1. **興奮性・抑制性ニューロンペア構造**
   - 入力層が興奮性（+1）と抑制性（-1）のペアで構成
   - 実際の脳のニューロンの性質を模倣
   - Dale's Principle（デールの原理）を厳密に適用

2. **アミン拡散による学習制御**
   - 出力層の誤差が「アミン濃度」として隠れ層に拡散
   - ドーパミン・セロトニンなどの神経伝達物質を模倣
   - 正誤差・負誤差の2種類のアミンで学習を制御

3. **誤差逆伝播を使わない**
   - 微分の連鎖律による誤差逆伝播法を使用しない
   - より生物学的に妥当な学習メカニズム
   - 局所的な学習規則のみで学習可能

### オリジナルED法の課題

オリジナルED法は優れた理論でしたが、以下の課題がありました：

- ✅ **二値分類には強い**: Yes/Noの判定など、2クラス分類で高性能
- ❌ **多クラス分類は不得手**: 3クラス以上の分類では性能が低下
- ❌ **独立重み空間の問題**: 各クラスごとに独立した重み行列を持つ実装では、クラス間の情報共有ができない

### 解決策：コラム構造の導入

本プロジェクトでは、**大脳皮質のコラム構造**を導入することで、この課題を解決しました。

**コラム構造とは**:
- 大脳皮質を垂直に貫く柱状の機能単位（直径約0.5mm）
- 特定の刺激に選択的に反応するニューロン群
- 隣接コラムは類似した刺激に反応（視覚野では「傾きの異なる線」など）

**本実装での応用**:
- 各出力クラスに対応する専用のニューロン群（コラム）を作成
- 1つの共有重み空間内で、コラム帰属度により各クラスを区別
- クラス間の情報共有と専門化を両立

```
従来のED法:           本実装（コラム構造導入）:
┌─────────┐        ┌─────────────────┐
│ クラス0  │        │   共有重み空間    │
│ 独立重み │        │ ┌───┬───┬───┐ │
└─────────┘        │ │C0 │C1 │C2 │ │  C0, C1, C2 = コラム
┌─────────┐        │ │   │   │   │ │  (各クラス専用)
│ クラス1  │   →    │ └───┴───┴───┘ │
│ 独立重み │        │  情報共有 + 専門化 │
└─────────┘        └─────────────────┘
┌─────────┐
│ クラス2  │
│ 独立重み │
└─────────┘
メモリ増大          メモリ効率的
情報共有なし        情報共有あり
```

### 本実装の位置付け

✅ **オリジナルED法の理論を100%保持**: アミン拡散、Dale's Principle、局所学習規則  
✅ **コラム構造で多クラス分類を実現**: 10クラスのMNISTで73.2%の精度  
✅ **生物学的妥当性を維持**: 脳の実際の構造とメカニズムに基づく  
✅ **拡張可能な設計**: 多層化、ミニバッチ、GPU対応などの現代的機能を追加

---

## 🚀 クイックスタート

### 環境要件

- Python 3.8以上
- NumPy
- TorchVision（データセット自動ダウンロード用）
- Matplotlib（可視化用、オプション）

### インストール手順

```bash
# リポジトリのクローン
git clone https://github.com/yourusername/column_ed_snn.git
cd column_ed_snn

# 仮想環境の作成（推奨）
python -m venv .venv
source .venv/bin/activate  # Windowsの場合: .venv\Scripts\activate

# 必要なパッケージのインストール
pip install numpy torchvision matplotlib
```

### 30秒で動かす

最小限のコマンドで学習を開始できます：

```bash
python columnar_ed_ann_v017_dale_fixed.py --train 500 --test 200 --epochs 5
```

このコマンドは以下を実行します：
- MNISTデータセットを自動ダウンロード（初回のみ）
- 500サンプルで訓練、200サンプルでテスト
- 5エポック学習
- 約1-2分で完了

### 期待される出力

```
データ読み込み中... (訓練:500, テスト:200)

ネットワーク初期化中...
  学習率: 0.05
  側方抑制強度: 0.1
  コラム係数: 0.3

======================================================================
学習開始 [v0.1.7 - Dale's Principle Fixed]
======================================================================
  ※重み更新時: sign_constraintを適用しない
  ※学習後: 重みの符号を強制（興奮性=正、抑制性=負）
======================================================================
Epoch   1: Train Acc=0.1460, Loss=2.3522 | Test Acc=0.1400, Loss=2.3878
Epoch   2: Train Acc=0.1540, Loss=2.3633 | Test Acc=0.2100, Loss=2.2701
Epoch   3: Train Acc=0.1760, Loss=2.2503 | Test Acc=0.2300, Loss=2.1752
Epoch   4: Train Acc=0.2160, Loss=2.1414 | Test Acc=0.2550, Loss=2.0896
Epoch   5: Train Acc=0.2880, Loss=2.0389 | Test Acc=0.2500, Loss=2.0650

学習完了！
```

---

## 📖 基本的な使い方

### 基本コマンド

```bash
python columnar_ed_ann_v017_dale_fixed.py [オプション]
```

### データセットの選択

デフォルトはMNISTですが、他のデータセットも使用できます：

```bash
# MNIST（デフォルト）
python columnar_ed_ann_v017_dale_fixed.py --train 3000 --test 1000

# Fashion-MNIST
python columnar_ed_ann_v017_dale_fixed.py --dataset fashion --train 3000 --test 1000
```

### 主要なオプション説明

#### 学習率（`--alpha`）

学習の速さを制御します。推奨値は0.05です。

```bash
# デフォルト（推奨）
python columnar_ed_ann_v017_dale_fixed.py --alpha 0.05

# より慎重に学習（収束は遅いが安定）
python columnar_ed_ann_v017_dale_fixed.py --alpha 0.03

# より速く学習（不安定になる可能性）
python columnar_ed_ann_v017_dale_fixed.py --alpha 0.1
```

**調整のヒント**:
- 精度が振動する → 学習率を下げる
- 収束が遅い → 学習率を上げる
- 通常は0.03〜0.1の範囲で調整

#### エポック数（`--epochs`）

学習を繰り返す回数です。

```bash
# クイックテスト
python columnar_ed_ann_v017_dale_fixed.py --epochs 5

# 標準的な学習
python columnar_ed_ann_v017_dale_fixed.py --epochs 10

# より長く学習（過学習に注意）
python columnar_ed_ann_v017_dale_fixed.py --epochs 20
```

#### 隠れ層のサイズ（`--hidden`）

ネットワークの表現力を制御します。

```bash
# シンプル（デフォルト）
python columnar_ed_ann_v017_dale_fixed.py --hidden 256

# より複雑なパターン認識
python columnar_ed_ann_v017_dale_fixed.py --hidden 512

# 軽量版
python columnar_ed_ann_v017_dale_fixed.py --hidden 128
```

#### コラム構造のパラメータ

```bash
# コラムニューロン数（各クラスの専用ニューロン数）
python columnar_ed_ann_v017_dale_fixed.py --column_neurons 25  # デフォルト

# コラム間の重複係数（情報共有の度合い）
python columnar_ed_ann_v017_dale_fixed.py --column_overlap 0.3  # デフォルト

# 側方抑制強度（クラス間競合の強さ）
python columnar_ed_ann_v017_dale_fixed.py --lateral_strength 0.1  # デフォルト
```

### 可視化オプション

学習過程をリアルタイムで可視化できます：

```bash
# 学習曲線と混同行列を表示
python columnar_ed_ann_v017_dale_fixed.py --viz

# 各層のニューロン活動をヒートマップ表示
python columnar_ed_ann_v017_dale_fixed.py --heatmap

# 両方を表示（2つのウィンドウ）
python columnar_ed_ann_v017_dale_fixed.py --viz --heatmap

# 図を保存
python columnar_ed_ann_v017_dale_fixed.py --viz --save_fig
```

### 実用的なコマンド例

```bash
# 1. クイックテスト（1-2分）
python columnar_ed_ann_v017_dale_fixed.py --train 500 --test 200 --epochs 5

# 2. 標準的な学習（5-10分）
python columnar_ed_ann_v017_dale_fixed.py --train 3000 --test 1000 --epochs 10 --viz

# 3. 高精度を目指す（15-30分）
python columnar_ed_ann_v017_dale_fixed.py --train 5000 --test 2000 --epochs 20 --alpha 0.03

# 4. パラメータチューニング
python columnar_ed_ann_v017_dale_fixed.py --train 3000 --test 1000 --epochs 10 \
  --alpha 0.05 --column_neurons 30 --column_overlap 0.3 --lateral_strength 0.1
```

---

## 🎯 プロジェクトの背景と目的

### ED法の歴史的意義

ED法は、1999年という早い時期に、金子勇氏によって提案された画期的な学習アルゴリズムです。

**当時としての先進性**:
- 生物学的妥当性への早期着目
- 神経科学的知見の工学的応用
- 誤差逆伝播に頼らない独創的アプローチ

**現代における重要性**:
- 局所学習の重要性が再認識されている
- エネルギー効率の高い学習への需要
- 脳型コンピューティングへの応用可能性

### 生物学的妥当性の重要性

**なぜ生物学的妥当性が重要か**:

1. **持続可能なAI**: 脳は約20Wの消費電力で動作、現代のAIは数十kW以上
2. **ロバスト性**: 生物の脳は部分的な損傷にも強い
3. **汎化性能**: 少ないデータから効率的に学習
4. **新しい学習原理**: 従来のAIにない新しい可能性

### 本プロジェクトが目指したもの

**主要な目標**:
1. オリジナルED法の理論的本質を100%保持
2. 多クラス分類問題への拡張
3. 実用的な精度の達成（70%以上）
4. 生物学的妥当性の維持

**克服した課題**:
- ❌ **独立重み空間の問題** → ✅ コラム構造による共有重み空間
- ❌ **クラス間情報共有の欠如** → ✅ コラム帰属度による制御
- ❌ **メモリ使用量の増大** → ✅ 効率的なメモリ管理
- ❌ **第1層の学習問題** → ✅ Dale's Principleの正しい実装

### 達成した成果と今後の展望

**達成した成果**:
- ✅ MNIST 10クラス分類で73.2%の精度
- ✅ 第1層を含む全層が正常に学習
- ✅ 生物学的妥当性を維持したまま多クラス分類を実現
- ✅ 詳細な技術ドキュメント（3つのmdファイル）

**今後の展望**:
- 🔬 より複雑なデータセット（CIFAR-10など）への適用
- 🔬 深層化（3層以上の隠れ層）
- 🔬 Spiking Neural Network（SNN）との統合
- 🔬 生物学的時間遅延の導入
- 🔬 ハードウェア実装への展開

---

## 🔧 技術的な詳細

このセクションは、アルゴリズムの内部動作を理解したい中級者向けです。

### アーキテクチャ

#### ネットワーク構造

```
入力層（784ピクセル）
    ↓ ペア構造で2倍に
入力層ペア（1568ニューロン）
    ↓ 全結合
隠れ層（256ニューロン）
    ├─ コラム0（クラス0専用、約25ニューロン）
    ├─ コラム1（クラス1専用、約25ニューロン）
    ├─ ...
    └─ コラム9（クラス9専用、約25ニューロン）
    ↓ 全結合
出力層（10ニューロン）
    └─ 各クラスの出力値
```

#### 入力層のペア構造

オリジナルED法の特徴である、興奮性・抑制性ニューロンペアを実装：

```python
def create_excitatory_inhibitory_pairs(x):
    """各ピクセル値を興奮性・抑制性ペアに複製"""
    # 784ピクセル → 1568ニューロン（各ピクセルが2つのニューロンに対応）
    paired_data = np.repeat(x, 2)
    return paired_data

# 例: [0.5, 0.8, 0.3] → [0.5, 0.5, 0.8, 0.8, 0.3, 0.3]
#     ピクセル値 → (興奮性, 抑制性) ペア
```

#### コラム構造の配置

各クラスに対応するニューロン群をガウス分布で配置：

```python
# コラム帰属度マップの例（10クラス × 256ニューロン）
column_affinity[0] = [0.9, 0.8, 0.3, 0.1, 0.0, ...]  # クラス0のコラム
column_affinity[1] = [0.1, 0.3, 0.8, 0.9, 0.8, ...]  # クラス1のコラム
column_affinity[2] = [0.0, 0.1, 0.3, 0.8, 0.9, ...]  # クラス2のコラム
...

# 各ニューロンは、主に1つのクラスに帰属するが、
# 他のクラスとも弱く結合（情報共有）
```

### 学習メカニズム

#### Winner-Takes-All方式

最も出力が大きいクラス（勝者）に基づいて学習戦略を決定：

```python
winner_class = np.argmax(z_output)  # 最大出力のクラス

if winner_class == y_true:
    # ケース1: 正解時
    # → 正解クラスのコラムをさらに強化
    learning_targets = [(y_true, excitatory_amine)]
else:
    # ケース2: 誤答時
    # → 勝者クラスを抑制、正解クラスを強化
    learning_targets = [
        (winner_class, inhibitory_amine),  # 勝者を抑制
        (y_true, excitatory_amine)         # 正解を強化
    ]
```

**特徴**:
- クラス間の競合学習
- 選択的な学習（全クラスを更新しない）
- 効率的な計算

#### アミン濃度計算

オリジナルED法の核心であるアミン拡散メカニズム：

```python
def calculate_amine_separation(error, amine_concentrations, class_idx, 
                                initial_amine=0.7):
    """
    誤差をアミン濃度に変換
    
    - 正誤差 → 興奮性アミン（type=0）
    - 負誤差 → 抑制性アミン（type=1）
    """
    if error > 0:
        amine_concentrations[class_idx, :, 0] = initial_amine * error
    else:
        amine_concentrations[class_idx, :, 1] = initial_amine * abs(error)
```

**側方抑制の効果**:
```python
# 誤答時、勝者からの抑制を考慮してアミン濃度を増強
lateral_effect = lateral_weights[winner_class, y_true]  # 負の値
if lateral_effect < 0:
    enhanced_amine = 0.7 * (1.0 - lateral_effect)  # 増強
    # 例: lateral_effect = -0.1 → enhanced_amine = 0.77
```

#### Dale's Principle（デールの原理）

生物学的制約：ニューロンは興奮性または抑制性のどちらか一方のみ。

**実装の重要ポイント**:
```python
# 初期化時: 絶対値を取ってから符号制約
ei_input = [+1, -1, +1, -1, ...]  # 興奮性・抑制性フラグ
sign_matrix = np.outer(ei_hidden, ei_input)
w_hidden[0] = np.abs(w_hidden[0]) * sign_matrix
# → 興奮性への重みは正、抑制性への重みは負

# 重み更新時: 符号制約を適用しない（自由に学習）
delta_w = amine_f * z_input  # sign_matrix を掛けない

# 学習後: 符号を強制
w_hidden[0] = np.abs(w_hidden[0]) * sign_matrix
```

**なぜこの方法か**:
- 重み更新時に符号制約を適用すると、興奮性と抑制性の変化が完全に相殺される
- 学習後に符号を強制することで、Dale's Principleを満たしつつ学習を可能にする
- 詳細は `コラム構造を用いたED法の実装方法.md` のセクション3を参照

### 拡張機能

本実装では、オリジナルED法を拡張した以下の機能を提供：

#### 多層対応

```bash
# 1層（デフォルト）
python columnar_ed_ann_v017_dale_fixed.py --hidden 256

# 2層（より複雑なパターン認識）
python columnar_ed_ann_v017_dale_fixed.py --hidden 256,128

# 3層（深層学習）
python columnar_ed_ann_v017_dale_fixed.py --hidden 512,256,128
```

#### ミニバッチ学習

```bash
# バッチサイズの調整
python columnar_ed_ann_v017_dale_fixed.py --batch_size 64  # デフォルト
python columnar_ed_ann_v017_dale_fixed.py --batch_size 32  # より細かい更新
python columnar_ed_ann_v017_dale_fixed.py --batch_size 128  # より高速
```

#### GPU対応（CuPy）

```bash
# CuPyがインストールされていれば自動的にGPUを使用
pip install cupy-cuda11x  # CUDA 11.x用

# CPUでも動作（自動フォールバック）
```

---

## 🎛️ パラメータチューニングガイド

### 基本パラメータ

| パラメータ | デフォルト | 推奨範囲 | 説明 |
|-----------|----------|---------|------|
| `--alpha` | 0.05 | 0.03-0.1 | 学習率 |
| `--epochs` | 10 | 5-20 | エポック数 |
| `--train` | 500 | 500-5000 | 訓練サンプル数 |
| `--test` | 200 | 200-2000 | テストサンプル数 |
| `--hidden` | 256 | 128-512 | 隠れ層ニューロン数 |
| `--batch_size` | 64 | 32-128 | ミニバッチサイズ |

### コラム構造パラメータ

| パラメータ | デフォルト | 推奨範囲 | 説明 |
|-----------|----------|---------|------|
| `--column_neurons` | 25 | 20-40 | 各クラスのニューロン数 |
| `--column_overlap` | 0.3 | 0.2-0.5 | コラム間の重複係数 |
| `--lateral_strength` | 0.1 | 0.05-0.15 | 側方抑制強度 |
| `--column_coeff` | 0.5 | 0.3-0.7 | コラム内学習係数 |
| `--non_column_coeff` | 1.5 | 1.2-2.0 | コラム外学習係数 |

### 推奨設定

#### 高精度重視

```bash
python columnar_ed_ann_v017_dale_fixed.py \
  --train 5000 --test 2000 --epochs 20 \
  --alpha 0.03 --hidden 512 \
  --column_neurons 30 --column_overlap 0.25
```

#### バランス型（推奨）

```bash
python columnar_ed_ann_v017_dale_fixed.py \
  --train 3000 --test 1000 --epochs 10 \
  --alpha 0.05 --hidden 256 \
  --column_neurons 25 --column_overlap 0.3
```

#### 高速実験

```bash
python columnar_ed_ann_v017_dale_fixed.py \
  --train 1000 --test 500 --epochs 5 \
  --alpha 0.1 --hidden 128 \
  --column_neurons 20
```

### トラブルシューティング

#### 問題: 精度が上がらない（< 30%）

**可能性のある原因と解決策**:

1. **学習率が高すぎる**
   ```bash
   # 解決策: 学習率を下げる
   --alpha 0.03  # デフォルトの0.05より低く
   ```

2. **エポック数が少なすぎる**
   ```bash
   # 解決策: より長く学習
   --epochs 15  # デフォルトの10より多く
   ```

3. **訓練データが少なすぎる**
   ```bash
   # 解決策: サンプル数を増やす
   --train 5000  # デフォルトの500より多く
   ```

#### 問題: 精度が振動する

**可能性のある原因と解決策**:

```bash
# 学習率を下げる
--alpha 0.03

# バッチサイズを大きくする（学習を安定化）
--batch_size 128
```

#### 問題: 過学習（訓練精度 >> テスト精度）

**可能性のある原因と解決策**:

```bash
# エポック数を減らす
--epochs 8

# 正則化を強化（側方抑制を強める）
--lateral_strength 0.15

# より多くのテストデータ
--test 2000
```

#### 問題: 学習が遅い

**可能性のある原因と解決策**:

```bash
# バッチサイズを大きくする
--batch_size 128

# 隠れ層を小さくする
--hidden 128

# GPU対応（CuPy）をインストール
pip install cupy-cuda11x
```

---

## 📊 実験と結果

### MNISTでの性能

**大規模テスト**（訓練3000、テスト1000、10エポック）:

```
最終結果:
  訓練精度: 79.0%
  テスト精度: 73.2%（最高、Epoch 9）
              72.6%（最終、Epoch 10）
```

**学習曲線の詳細**:

| Epoch | 訓練精度 | 訓練Loss | テスト精度 | テストLoss |
|-------|---------|---------|-----------|-----------|
| 1 | 21.4% | 2.1663 | 25.9% | 1.9293 |
| 2 | 45.0% | 1.6700 | 53.2% | 1.5589 |
| 3 | 58.4% | 1.3487 | 56.6% | 1.3581 |
| 4 | 65.2% | 1.1647 | 62.4% | 1.1629 |
| 5 | 70.6% | 1.0373 | 64.3% | 1.0920 |
| 6 | 73.5% | 0.9608 | 67.7% | 1.0945 |
| 7 | 75.3% | 0.9019 | 71.5% | 0.9985 |
| 8 | 76.9% | 0.8571 | 71.7% | 0.9461 |
| 9 | 78.2% | 0.8264 | **73.2%** | 0.9260 |
| 10 | 79.0% | 0.8039 | 72.6% | 0.9035 |

**観察**:
- 初期エポックで急速に学習（1→2エポックで+27.8%）
- エポック5以降は緩やかに改善
- エポック9で最高テスト精度73.2%を達成
- 若干の過学習傾向（訓練79.0% vs テスト72.6%）

### 学習曲線の例

```
訓練精度・テスト精度の推移:

80% ┤                                   ●
    │                               ●
70% ┤                           ●
    │                       ●
60% ┤                   ●               訓練: ●
    │               ●                   テスト: ○
50% ┤           ○
    │       ●
40% ┤   ●
    │
30% ┤
    │○
20% ┤●
    └─┬───┬───┬───┬───┬───┬───┬───┬───┬───┬─
      1   2   3   4   5   6   7   8   9  10 (Epoch)
```

### バージョン別比較

実装の進化と精度向上の軌跡：

| バージョン | 訓練精度 | テスト精度 | 第1層学習 | 主な特徴 |
|-----------|---------|-----------|----------|---------|
| v0.1.3 | 28.4% | - | ❌ なし | 第2層以降のみ学習 |
| v0.1.4 | - | 6-14% | ❌ なし | Broadcastingミス |
| v0.1.5 | 63.5% | 46.5% | ❌ なし | v0.1.3と同等 |
| v0.1.6 | 37.4% | 46.5% | ❌ 相殺 | 初期化のみ修正 |
| **v0.1.7** | **79.0%** | **73.2%** | ✅ **機能** | **Dale's Principle修正** |

**重要な発見**:
- v0.1.6まで：第1層の重みは変化するが、出力が不変（相殺問題）
- v0.1.7で解決：重み更新時に符号制約を適用せず、学習後に符号を強制
- 結果：第1層が正常に学習し、精度が大幅に向上（46.5% → 73.2%）

### 他手法との比較

**参考：一般的な手法のMNIST性能**:

| 手法 | テスト精度 | 特徴 |
|------|-----------|------|
| 線形分類器 | ~92% | 最もシンプル |
| 2層NN（誤差逆伝播） | ~95% | 標準的な方法 |
| 深層NN | ~98% | より複雑な構造 |
| CNN | ~99% | 画像認識に最適 |
| **本実装（ED法+コラム）** | **73.2%** | **生物学的妥当性を重視** |

**注意**:
- 本実装の目的は最高精度の追求ではなく、生物学的妥当性の保持
- 誤差逆伝播を使わず、局所学習のみで73.2%は意義深い
- パラメータチューニングやデータ量増加でさらなる改善の余地あり

---

## 📁 ファイル構成

### 主要ファイル

```
column_ed_snn/
├── columnar_ed_ann_v017_dale_fixed.py  # メインの実行ファイル（最新版）
├── columnar_ed_ann_v011_wta_lateral.py # 基底クラス（WTA+側方抑制）
├── ed_functions_patch.py               # オリジナルED法の関数群
├── README.md                            # 本ファイル
└── requirements.txt                     # 依存パッケージリスト
```

**バージョン履歴**:
- `v0.1.7`: Dale's Principle修正版（現在の最新・推奨）
- `v0.1.6`: 重み初期化修正版
- `v0.1.5`: v0.1.3ベース
- `v0.1.4`: 理論準拠版（Broadcastingミス）
- `v0.1.3`: Dale's Principle固定版
- `v0.1.1`: WTA+側方抑制版

### ドキュメント

```
docs/
├── コラム構造を用いたED法の実装方法.md  # 実装ガイド（初学者向け）
├── columnar_ed.prompt.md                # 完全仕様書（開発者向け）
└── ED法_解説資料.md                      # ED法の基本理論
```

**ドキュメントの使い分け**:
- **初めての方**: `README.md`（本ファイル）→ `コラム構造を用いたED法の実装方法.md`
- **実装を理解したい方**: `コラム構造を用いたED法の実装方法.md`
- **理論を深く学びたい方**: `ED法_解説資料.md` → `columnar_ed.prompt.md`
- **開発者**: `columnar_ed.prompt.md`

### ユーティリティ

```
column_ed_snn/
├── debug_v017_layer1.py                # 第1層の学習確認スクリプト
├── debug_v013_output_nonchange.py      # 相殺問題の検証スクリプト
└── grid_search_results/                # パラメータ探索結果（実験データ）
```

---

## 📚 さらに学ぶために

### 詳細ドキュメント

本プロジェクトには、3つの詳細ドキュメントがあります：

#### 1. コラム構造を用いたED法の実装方法.md

**対象**: 実装を理解したい方、再実装したい方  
**内容**:
- コラム構造の作成方法（完全なコード例）
- ED法の実装手順
- Dale's Principleの正しい実装（相殺問題の解決）
- Winner-Takes-All方式の実装
- 側方抑制の実装
- デバッグとトラブルシューティング

**読むべき順序**: セクション1→3→4→2→5...

#### 2. ED法_解説資料.md

**対象**: ED法の理論を深く学びたい方  
**内容**:
- アミン拡散メカニズムの詳細
- 興奮性・抑制性ニューロンの役割
- Dale's Principleの理論的背景
- オリジナルED法のC実装の解説

#### 3. columnar_ed.prompt.md

**対象**: 開発者、研究者  
**内容**:
- 完全な技術仕様
- データ構造の定義
- 全パラメータの詳細
- 拡張機能の一覧
- 生物学的妥当性の根拠
- 実装ガイドライン

### 理論的背景

**ED法の理論的基礎**:
- 金子勇 (1999). "Error Diffusion Learning Algorithm"
- アミン神経系による学習制御
- Dale's Principleと神経科学

**コラム構造の生物学的背景**:
- 大脳皮質の柱状構造
- 視覚野（V1, V2）の機能マップ
- 特徴抽出と階層的処理

**関連する現代の研究**:
- 生物学的に妥当なニューラルネットワーク
- Spiking Neural Networks (SNN)
- 局所学習とHebbianルール
- エネルギー効率の高いAI

### 関連論文・資料

**推奨文献**（理論的理解のため）:

1. **神経科学の基礎**
   - Dale's Principle
   - 神経伝達物質（ドーパミン、セロトニン）
   - 大脳皮質の構造

2. **機械学習の理論**
   - 誤差逆伝播法（対比のため）
   - 競合学習とWinner-Takes-All
   - 局所学習規則

3. **生物学的AI**
   - Spiking Neural Networks
   - Neuromorphic Computing
   - Brain-inspired AI

---

## 🤝 貢献とライセンス

### 貢献方法

このプロジェクトへの貢献を歓迎します！

**貢献の方法**:
1. Issueの報告（バグ、機能要望）
2. Pull Requestの提出
3. ドキュメントの改善
4. 実験結果の共有

**開発の方針**:
- オリジナルED法の理論を尊重
- 生物学的妥当性を維持
- コードの可読性を重視
- 詳細なドキュメント

### ライセンス情報

**本実装**: MIT License

**オリジナルED法**: 金子勇氏によるオリジナル理論（1999年）

このプロジェクトは、金子勇氏の独創的な理論を尊重し、その本質を保ちながら、現代的な実装と拡張を行っています。

### 謝辞

**金子勇氏への敬意**

本プロジェクトは、金子勇氏が1999年に提案したED法（Error Diffusion Learning Algorithm）の理論に基づいています。

1999年という早い時期に、生物学的妥当性を重視した独創的な学習アルゴリズムを提案された金子氏の先見性と業績に、深い敬意を表します。

本実装では、氏の理論の本質を100%保持しながら、コラム構造の導入により多クラス分類への拡張を実現しました。これは、金子氏の優れた理論的基盤があってこそ可能になったものです。

---

## ❓ FAQ（よくある質問）

### ED法について

**Q1: ED法と通常のニューラルネットワークの最大の違いは？**

A: 最大の違いは**誤差逆伝播を使わない**ことです。通常のNNは微分の連鎖律で誤差を逆伝播しますが、ED法はアミン拡散というより生物学的なメカニズムで学習します。

**Q2: なぜ生物学的妥当性が重要なのですか？**

A: 
1. **エネルギー効率**: 脳は約20Wで動作、現代のAIは数十kW以上
2. **新しい原理**: 従来のAIにない学習メカニズムの発見
3. **ロバスト性**: 部分的な損傷に強い
4. **汎化性能**: 少ないデータから効率的に学習

**Q3: Dale's Principleとは何ですか？**

A: ニューロンは興奮性（正の出力）または抑制性（負の出力）のどちらか一方の性質のみを持つ、という生物学的原理です。本実装では、重みの符号制約として厳密に実装しています。

### コラム構造について

**Q4: コラム構造を導入したことで何が変わりましたか？**

A: オリジナルED法は二値分類が得意でしたが、多クラス分類は不得手でした。コラム構造の導入により：
- 各クラスに専用のニューロン群を割り当て
- 1つの共有重み空間で効率的に多クラス分類
- クラス間の情報共有と専門化を両立

**Q5: コラム構造は実際の脳の構造と同じですか？**

A: 基本的な概念は同じです：
- 大脳皮質の柱状構造を模倣
- 特定の刺激に選択的に反応
- 隣接コラムは類似した刺激に反応

ただし、本実装は簡略化されたモデルで、実際の脳の複雑さをすべて再現しているわけではありません。

**Q6: `column_neurons`のパラメータはどう決めればいいですか？**

A: デフォルトの25が推奨ですが、以下を参考に調整できます：
- 小さい（< 20）: 表現力不足、精度低下
- 適切（20-40）: バランスが良い
- 大きい（> 40）: 過学習、コラム間干渉

### 実装について

**Q7: 第1層が学習しない問題はどう解決しましたか？**

A: Dale's Principleの実装方法を変更しました：
- **従来（v0.1.6まで）**: 重み更新時に符号制約を適用 → 相殺が発生
- **v0.1.7**: 重み更新時は自由に学習、学習後に符号を強制 → 相殺を回避

詳細は `コラム構造を用いたED法の実装方法.md` のセクション3を参照してください。

**Q8: 精度を向上させるにはどうすればいいですか？**

A: 以下を試してください：
1. 訓練データを増やす（`--train 5000`）
2. エポック数を増やす（`--epochs 20`）
3. 学習率を調整（`--alpha 0.03`）
4. 隠れ層を大きくする（`--hidden 512`）
5. パラメータチューニング（グリッドサーチ）

**Q9: GPUを使うにはどうすればいいですか？**

A: CuPyをインストールするだけです：
```bash
pip install cupy-cuda11x  # CUDA 11.x用
```
プログラムは自動的にGPUを検出して使用します。

**Q10: 他のデータセットで使えますか？**

A: はい、以下のデータセットをサポートしています：
- MNIST（デフォルト）
- Fashion-MNIST（`--dataset fashion`）
- 将来的にCIFAR-10などにも対応予定

### トラブルシューティング

**Q11: "CUDA out of memory"エラーが出ます**

A: 以下を試してください：
```bash
# バッチサイズを小さくする
--batch_size 32

# 隠れ層を小さくする
--hidden 128

# CPUで実行（CuPyをアンインストール）
pip uninstall cupy
```

**Q12: 精度が10%以下で停滞します**

A: 以下を確認してください：
1. 学習率が高すぎないか（`--alpha 0.03`を試す）
2. エポック数が十分か（`--epochs 15`以上）
3. データが正しく読み込まれているか（初回はダウンロードに時間がかかる）

**Q13: 可視化ウィンドウが表示されません**

A: 以下を確認してください：
```bash
# matplotlibがインストールされているか
pip install matplotlib

# バックエンドの設定（必要に応じて）
export MPLBACKEND=TkAgg

# SSHで接続している場合はX11フォワーディング
ssh -X user@host
```

**Q14: 学習が非常に遅いです**

A: 以下を試してください：
1. GPUを使用（CuPyインストール）
2. バッチサイズを大きくする（`--batch_size 128`）
3. サンプル数を減らす（テスト用: `--train 1000`）
4. 隠れ層を小さくする（`--hidden 128`）

**Q15: 異なる結果が毎回出ます（再現性がない）**

A: ランダムシードを固定してください：
```python
# プログラムの最初に追加
import numpy as np
np.random.seed(42)
```

---

## 📧 連絡先とサポート

**プロジェクトリポジトリ**: [GitHub](https://github.com/yourusername/column_ed_snn)

**Issue報告**: [GitHub Issues](https://github.com/yourusername/column_ed_snn/issues)

**質問・議論**: [GitHub Discussions](https://github.com/yourusername/column_ed_snn/discussions)

**開発者**: yoiwa0714

---

**最終更新**: 2025年11月30日  
**バージョン**: v0.1.7 (Dale's Principle Fixed)  
**ドキュメントバージョン**: 1.0.0
