# コラム構造ED法 グリッドサーチ戦略
## 目標：テスト精度 70-80%台達成

作成日: 2025-12-06

## 現状分析

### 現在の精度
- **2層（base_radius=1.0, 100 epochs）**: 最高 31.9% (現状最良)
- **3層（base_radius=1.0, 30 epochs）**: 最高 25.8%

### 課題
現在の精度（30%前後）から70%台への飛躍的向上が必要。
従来の微調整では不十分で、**パラメータの大幅な探索が必要**。

---

## グリッドサーチ対象パラメータの選定

### 【最優先】Phase 1: コア学習パラメータ

#### 1. **learning_rate（学習率）** - 最重要 ⭐⭐⭐⭐⭐
**現状**: 0.05（固定）

**問題点**:
- 過去の実装（コラムなし）の推奨値（0.03-0.1）をそのまま使用
- コラム構造導入後の最適値が未探索
- アミン拡散との相互作用が検証されていない

**提案範囲**:
```python
learning_rate: [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2]
# 理由:
# - 0.01-0.05: 安定した学習（現状の下限拡張）
# - 0.08-0.15: 高速学習の可能性
# - 0.2: 上限探索（過学習リスク含め検証）
```

**期待効果**: 学習速度と安定性の最適バランス発見（±10-20%精度向上期待）

---

#### 2. **u1（出力層→最終隠れ層のアミン拡散係数）** - 重要 ⭐⭐⭐⭐⭐
**現状**: 0.8（固定）

**問題点**:
- 過去の探索範囲（0.5-1.0）を実験せずに0.8採用
- 出力層の誤差伝播強度が最適化されていない
- コラム構造との相互作用が未検証

**提案範囲**:
```python
u1: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
# 理由:
# - 0.5-0.7: 弱い拡散（出力層重視）
# - 0.8: 現状ベースライン
# - 0.9-1.0: 強い拡散（隠れ層重視）
```

**期待効果**: 出力層と隠れ層の誤差バランス最適化（±5-15%精度向上期待）

---

#### 3. **lateral_lr（側方抑制学習率）** - 重要 ⭐⭐⭐⭐
**現状**: 0.05（固定）

**問題点**:
- 仕様書推奨範囲（0.05-0.15）の上限を試していない
- クラス間競合の強度が最適化されていない

**提案範囲**:
```python
lateral_lr: [0.03, 0.05, 0.08, 0.1, 0.15]
# 理由:
# - 0.03: 弱い側方抑制（クラス独立性高）
# - 0.05: 現状ベースライン
# - 0.08-0.15: 強い側方抑制（クラス競合強化）
```

**期待効果**: クラス分離性の向上（±5-10%精度向上期待）

---

### 【高優先】Phase 2: コラム構造パラメータ

#### 4. **base_column_radius（基準コラム半径）** - 重要 ⭐⭐⭐⭐
**現状**: 1.0（最適化済み、base=1.2から変更）

**実験済み結果**:
- base=1.2: 30.4%、飽和2120回
- base=1.0: 31.9%、飽和39回 ← 現状最良
- base=0.9: 29.6%、過学習

**提案範囲**:
```python
base_column_radius: [0.8, 0.9, 1.0, 1.1, 1.2, 1.5]
# 理由:
# - 0.8: さらに狭い範囲（特化性強化）
# - 0.9-1.1: 最適値周辺の微調整
# - 1.2-1.5: 広い範囲（汎化性強化）
```

**期待効果**: コラム内結合の最適化（±3-8%精度向上期待）

---

#### 5. **participation_rate（コラム参加率）** - 重要 ⭐⭐⭐
**現状**: 1.0（全ニューロン参加）

**問題点**:
- 全ニューロンが全クラスに関与
- 専門化（スペシャライゼーション）が不十分な可能性

**提案範囲**:
```python
participation_rate: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
# 理由:
# - 0.5-0.7: 強い専門化（各クラスに特化）
# - 0.8-0.9: 中程度の専門化
# - 1.0: 現状（全参加）
```

**期待効果**: ニューロンの専門化促進（±5-10%精度向上期待）

---

### 【中優先】Phase 3: 重み初期化パラメータ

#### 6. **w1（重み初期化範囲）** - 中程度 ⭐⭐⭐
**現状**: 1.0（固定）

**問題点**:
- 層ごとの調整係数（0.3x, 0.5x等）はあるが、基準値が最適化されていない

**提案範囲**:
```python
w1: [0.5, 0.8, 1.0, 1.5, 2.0]
# 理由:
# - 0.5-0.8: 小さい初期値（安定学習）
# - 1.0: 現状ベースライン
# - 1.5-2.0: 大きい初期値（高速学習）
```

**期待効果**: 初期学習の安定性向上（±3-5%精度向上期待）

---

#### 7. **u2（隠れ層間のアミン拡散係数）** - 中程度 ⭐⭐⭐
**現状**: 0.8（固定）

**問題点**:
- 多層構成での層間誤差伝播が未最適化
- 3層実験でLayer 0の学習困難（25.8%）

**提案範囲**:
```python
u2: [0.6, 0.7, 0.8, 0.9, 1.0]
# 理由:
# - 0.6-0.7: 弱い層間拡散
# - 0.8: 現状ベースライン
# - 0.9-1.0: 強い層間拡散（深層学習支援）
```

**期待効果**: 多層構成での学習改善（±3-8%精度向上期待、特に3層以上）

---

### 【低優先】Phase 4: アーキテクチャパラメータ

#### 8. **hidden_layers（隠れ層構成）** - 低優先 ⭐⭐
**現状**: [256, 128] or [256, 128, 64]

**提案範囲**:
```python
hidden_layers: [
    [256],           # 1層（シンプル）
    [256, 128],      # 2層（現状最良）
    [512, 256],      # 2層（大規模）
    [256, 128, 64],  # 3層（現状）
    [384, 192],      # 2層（中規模）
]
# 理由: 層数・サイズの組み合わせ探索
```

**期待効果**: アーキテクチャの最適化（±5-10%精度向上期待）

---

## グリッドサーチ実施計画

### フェーズ分割戦略

#### **Phase 1: コア学習パラメータ（最優先）**
**目標**: 基本学習性能の大幅向上

**パラメータ**:
- learning_rate: [0.01, 0.03, 0.05, 0.08, 0.1, 0.15, 0.2] (7値)
- u1: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (6値)
- lateral_lr: [0.03, 0.05, 0.08, 0.1, 0.15] (5値)

**探索空間**: 7 × 6 × 5 = **210通り**

**実行設定**:
```python
--train 1000 --test 1000 --epochs 20
# 理由: 短エポックで傾向把握、精度向上が見えたら延長
```

**期待効果**: 30% → 50-60%（±20-30%向上）

---

#### **Phase 2: コラム構造最適化**
**Phase 1の最良パラメータを固定して実施**

**パラメータ**:
- base_column_radius: [0.8, 0.9, 1.0, 1.1, 1.2, 1.5] (6値)
- participation_rate: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (6値)

**探索空間**: 6 × 6 = **36通り**

**実行設定**:
```python
--train 2000 --test 2000 --epochs 30
# Phase 1の最良設定を使用
```

**期待効果**: 50-60% → 60-70%（±10%向上）

---

#### **Phase 3: 精密チューニング**
**Phase 1-2の最良パラメータを固定して実施**

**パラメータ**:
- w1: [0.5, 0.8, 1.0, 1.5, 2.0] (5値)
- u2: [0.6, 0.7, 0.8, 0.9, 1.0] (5値)

**探索空間**: 5 × 5 = **25通り**

**実行設定**:
```python
--train 5000 --test 2000 --epochs 50
# 長時間学習で真の性能評価
```

**期待効果**: 60-70% → 70-80%（±10%向上）

---

## グリッドサーチ実装方針

### 1. スクリプト構造
```python
grid_search_phase1.py  # コア学習パラメータ
grid_search_phase2.py  # コラム構造パラメータ
grid_search_phase3.py  # 精密チューニング
```

### 2. 結果記録
```
results/
├── phase1/
│   ├── experiment_001.json  # lr=0.01, u1=0.5, lateral_lr=0.03
│   ├── experiment_002.json
│   └── ...
├── phase2/
└── phase3/
```

### 3. 自動分析
- 各フェーズ終了後、上位10設定を抽出
- ヒートマップ可視化（パラメータ相関）
- 最良設定の自動レポート生成

---

## 総合戦略サマリー

| Phase | パラメータ数 | 組み合わせ | エポック | 推定時間 | 期待精度 |
|-------|-------------|-----------|----------|----------|----------|
| Phase 1 | 3 | 210通り | 20 | ~12時間 | 50-60% |
| Phase 2 | 2 | 36通り | 30 | ~4時間 | 60-70% |
| Phase 3 | 2 | 25通り | 50 | ~5時間 | 70-80% |
| **合計** | **7** | **271通り** | - | **~21時間** | **70-80%** |

---

## 重要な注意事項

### 1. ED法の制約遵守
- **誤差逆伝播法禁止**: 微分の連鎖律を使用しない
- **純粋なED法**: `abs(z) × (1 - abs(z))` のみ使用
- **アミン拡散**: 生物学的メカニズムを維持

### 2. 過学習対策
- 訓練精度 vs テスト精度の監視
- 早期終了（early stopping）の検討
- 正則化は**ED法の範囲内**で実施

### 3. 結果検証
- 各フェーズで複数回実行（ランダムシード変更）
- 統計的有意性の確認
- 最良設定の再現性検証

---

## 次のアクション

1. **Phase 1グリッドサーチスクリプト作成**
   - learning_rate, u1, lateral_lr の3次元探索
   - 自動ログ記録とJSON出力

2. **実験環境の準備**
   - GPU/CPUリソース確認
   - 長時間実行の安定性確認

3. **ベースライン測定**
   - 現状設定（lr=0.05, u1=0.8, lateral_lr=0.05）で複数回実行
   - 平均精度と標準偏差を記録

**推奨開始**: Phase 1 から実施（最も効果が高い）
